author,publisher,title,url,year,abstract,session,pdf_url,openreview_url
"['Ching Fang', 'Kimberly Stachenfeld']",ICLR,Predictive auxiliary objectives in deep RL mimic learning in the brain,https://iclr.cc/virtual/2024/oral/19748,2024," The ability to predict upcoming events has been hypothesized to comprise a key aspect of natural and machine cognition. This is supported by trends in deep reinforcement learning (RL), where self-supervised auxiliary objectives such as prediction are widely used to support representation learning and improve task performance. Here, we study the effects predictive auxiliary objectives have on representation learning across different modules of an RL system and how these mimic representational changes observed in the brain. We find that predictive objectives improve and stabilize learning particularly in resource-limited architectures, and we identify settings where longer predictive horizons better support representational transfer. Furthermore, we find that representational changes in this RL system bear a striking resemblance to changes in neural activity observed in the brain across various experiments. Specifically, we draw a connection between the auxiliary predictive model of the RL system and hippocampus, an area thought to learn a predictive model to support memory-guided behavior. We also connect the encoder network and the value learning network of the RL system to visual cortex and striatum in the brain, respectively. This work demonstrates how representation learning in deep RL systems can provide an interpretable framework for modeling multi-region interactions in the brain. The deep RL perspective taken here also suggests an additional role of the hippocampus in the brain-- that of an auxiliary learning system that benefits representation learning in other regions.",Oral 1A,https://openreview.net/pdf?id=agPpmEgf8C,https://openreview.net/forum?id=agPpmEgf8C
"['HAOYUE DAI', 'Ignavier Ng', 'Gongxu Luo', 'Peter Spirtes', 'Petar Stojanov', 'Kun Zhang']",ICLR,Gene Regulatory Network Inference in the Presence of Dropouts: a Causal View,https://iclr.cc/virtual/2024/oral/19739,2024," Gene regulatory network inference (GRNI) is a challenging problem, particularly owing to the presence of zeros in single-cell RNA sequencing data: some are biological zeros representing no gene expression, while some others are technical zeros arising from the sequencing procedure (aka dropouts), which may bias GRNI by distorting the joint distribution of the measured gene expressions. Existing approaches typically handle dropout error via imputation, which may introduce spurious relations as the true joint distribution is generally unidentifiable. To tackle this issue, we introduce a causal graphical model to characterize the dropout mechanism, namely, Causal Dropout Model. We provide a simple yet effective theoretical result: interestingly, the conditional independence (CI) relations in the data with dropouts, after deleting the samples with zero values (regardless if technical or not) for the conditioned variables, are asymptotically identical to the CI relations in the original data without dropouts. This particular test-wise deletion procedure, in which we perform CI tests on the samples without zeros for the conditioned variables, can be seamlessly integrated with existing structure learning approaches including constraint-based and greedy score-based methods, thus giving rise to a principled framework for GRNI in the presence of dropouts. We further show that the causal dropout model can be validated from data, and many existing statistical models to handle dropouts fit into our model as specific parametric instances. Empirical evaluation on synthetic, curated, and real-world experimental transcriptomic data comprehensively demonstrate the efficacy of our method.",Oral 1D,https://openreview.net/pdf?id=gFR4QwK53h,https://openreview.net/forum?id=gFR4QwK53h
"['Izzeddin Gur', 'Hiroki Furuta', 'Austin Huang', 'Mustafa Safdari', 'Yutaka Matsuo', 'Douglas Eck', 'Aleksandra Faust']",ICLR,"A Real-World WebAgent with Planning, Long Context Understanding, and Program Synthesis",https://iclr.cc/virtual/2024/oral/19785,2024," Pre-trained large language models (LLMs) have recently achieved better generalization and sample efficiency in autonomous web automation.However, the performance on real-world websites has still suffered from (1) open domainness, (2) limited context length, and (3) lack of inductive bias on HTML.We introduce WebAgent, an LLM-driven agent that learns from self-experience to complete tasks on real websites following natural language instructions.WebAgent plans ahead by decomposing instructions into canonical sub-instructions, summarizes long HTML documents into task-relevant snippets, and acts on websites via Python programs generated from those.We design WebAgent with Flan-U-PaLM, for grounded code generation, and HTML-T5, new pre-trained LLMs for long HTML documents using local and global attention mechanisms and a mixture of long-span denoising objectives, for planning and summarization.We empirically demonstrate that our modular recipe improves the success on real websites by over 50%, and that HTML-T5 is the best model to solve various HTML understanding tasks; achieving 18.7% higher success rate than the prior method on MiniWoB web automation benchmark, and SoTA performance on Mind2Web, an offline task planning evaluation.",Oral 1B,https://openreview.net/pdf?id=9JQtrumvg8,https://openreview.net/forum?id=9JQtrumvg8
"['Yukang Chen', 'Shengju Qian', 'Haotian Tang', 'Xin Lai', 'Zhijian Liu', 'Song Han', 'Jiaya Jia']",ICLR,LongLoRA: Efficient Fine-tuning of Long-Context Large Language Models,https://iclr.cc/virtual/2024/oral/19790,2024," We present LongLoRA, an efficient fine-tuning approach that extends the context sizes of pre-trained large language models (LLMs), with limited computation cost.Typically, training LLMs with long context sizes is computationally expensive, requiring extensive training hours and GPU resources. For example, training on the context length of 8192 needs 16x computational costs in self-attention layers as that of 2048. In this paper, we speed up the context extension of LLMs in two aspects. On the one hand, although dense global attention is needed during inference, fine-tuning the model can be effectively and efficiently done by sparse local attention. The proposed shifted sparse attention effectively enables context extension, leading to non-trivial computation saving with similar performance to fine-tuning with vanilla attention. Particularly, it can be implemented with only two lines of code in training, while being optional in inference. On the other hand, we revisit the parameter-efficient fine-tuning regime for context expansion. Notably, we find that LoRA for context extension works well under the premise of trainable embedding and normalization. LongLoRA combines this improved LoRA with S^2-Attn. LongLoRA demonstrates strong empirical results on various tasks on Llama2 models from 7B/13B to 70B. LongLoRA extends Llama2 7B from 4k context to 100k, or Llama2 70B to 32k on a single 8x A100 machine. LongLoRA extends models' context while retaining their original architectures, and is compatible with most existing techniques, like Flash-Attention2. In addition, we further conduct supervised fine-tuning with LongLoRA and our long instruction-following LongAlpaca dataset. All our code, models, dataset, and demo are available at https://github.com/dvlab-research/LongLoRA.",Oral 1C,https://openreview.net/pdf?id=6PmJoRfdaK,https://openreview.net/forum?id=6PmJoRfdaK
"['Sirui Hong', 'Mingchen Zhuge', 'Jonathan Chen', 'Xiawu Zheng', 'Yuheng Cheng', 'Jinlin Wang', 'Ceyao Zhang', 'zili wang', 'Steven Yau', 'Zijuan Lin', 'Liyang Zhou', 'Chenyu Ran', 'Lingfeng Xiao', 'Chenglin Wu', 'Jürgen Schmidhuber']",ICLR,MetaGPT: Meta Programming for A Multi-Agent Collaborative Framework,https://iclr.cc/virtual/2024/oral/19756,2024," Recently, remarkable progress has been made on automated problem solving through societies of agents based on large language models (LLMs). Previous LLM-based multi-agent systems can already solve simple dialogue tasks. More complex tasks, however, face challenges through logic inconsistencies due to cascading hallucinations caused by naively chaining LLMs. Here we introduce MetaGPT, an innovative meta-programming framework incorporating efficient human workflows into LLM-based multi-agent collaborations. MetaGPT encodes Standardized Operating Procedures (SOPs) into prompt sequences for more streamlined workflows, thus allowing agents with human-like domain expertise to verify intermediate results and reduce errors.  MetaGPT utilizes an assembly line paradigm to assign diverse roles to various agents, efficiently breaking down complex tasks into subtasks involving many agents working together.  On collaborative software engineering benchmarks, MetaGPT generates more coherent solutions than previous chat-based multi-agent systems.",Oral 2A,https://openreview.net/pdf?id=VtmBAGCN7o,https://openreview.net/forum?id=VtmBAGCN7o
"['Timothée Darcet', 'Maxime Oquab', 'Julien Mairal', 'Piotr Bojanowski']",ICLR,Vision Transformers Need Registers,https://iclr.cc/virtual/2024/oral/19794,2024," Transformers have recently emerged as a powerful tool for learning visual representations. In this paper, we identify and characterize artifacts in feature maps of both supervised and self-supervised ViT networks. The artifacts correspond to high-norm tokens appearing during inference primarily in low-informative background areas of images, that are repurposed for internal computations. We propose a simple yet effective solution based on providing additional tokens to the input sequence of the Vision Transformer to fill that role. We show that this solution fixes that problem entirely for both supervised and self-supervised models, sets a new state of the art for self-supervised visual models on dense visual prediction tasks, enables object discovery methods with larger models, and most importantly leads to smoother feature maps and attention maps for downstream visual processing.",Oral 2B,https://openreview.net/pdf?id=2dnO3LLiJ1,https://openreview.net/forum?id=2dnO3LLiJ1
"['Zhantao Yang', 'Ruili Feng', 'Han Zhang', 'Yujun Shen', 'Kai Zhu', 'Lianghua Huang', 'Yifei Zhang', 'Yu Liu', 'Deli Zhao', 'Jingren Zhou', 'Fan Cheng']",ICLR,Lipschitz Singularities in Diffusion Models,https://iclr.cc/virtual/2024/oral/19755,2024," Diffusion models, which employ stochastic differential equations to sample images through integrals, have emerged as a dominant class of generative models. However, the rationality of the diffusion process itself receives limited attention, leaving the question of whether the problem is well-posed and well-conditioned. In this paper, we uncover a vexing propensity of diffusion models: they frequently exhibit the infinite Lipschitz near the zero point of timesteps. We provide theoretical proofs to illustrate the presence of infinite Lipschitz constants and empirical results to confirm it. The Lipschitz singularities pose a threat to the stability and accuracy during both the training and inference processes of diffusion models. Therefore, the mitigation of Lipschitz singularities holds great potential for enhancing the performance of diffusion models. To address this challenge, we propose a novel approach, dubbed E-TSDM, which alleviates the Lipschitz singularities of the diffusion model near the zero point. Remarkably, our technique yields a substantial improvement in performance. Moreover, as a byproduct of our method, we achieve a dramatic reduction in the Fréchet Inception Distance of acceleration methods relying on network Lipschitz, including DDIM and DPM-Solver, by over 33\%. Extensive experiments on diverse datasets validate our theory and method. Our work may advance the understanding of the general diffusion process, and also provide insights for the design of diffusion models.",Oral 2C,https://openreview.net/pdf?id=WNkW0cOwiz,https://openreview.net/forum?id=WNkW0cOwiz
"['Bohang Zhang', 'Jingchu Gai', 'Yiheng Du', 'Qiwei Ye', 'Di He', 'Liwei Wang']",ICLR,Beyond Weisfeiler-Lehman: A Quantitative Framework for GNN Expressiveness,https://iclr.cc/virtual/2024/oral/19773,2024," Designing expressive Graph Neural Networks (GNNs) is a fundamental topic in the graph learning community. So far, GNN expressiveness has been primarily assessed via the Weisfeiler-Lehman (WL) hierarchy. However, such an expressivity measure has notable limitations: it is inherently coarse, qualitative, and may not well reflect practical requirements (e.g., the ability to encode substructures). In this paper, we introduce a novel framework for quantitatively studying the expressiveness of GNN architectures, addressing all the above limitations. Specifically, we identify a fundamental expressivity measure termed homomorphism expressivity, which quantifies the ability of GNN models to count graphs under homomorphism. Homomorphism expressivity offers a complete and practical assessment tool: the completeness enables direct expressivity comparisons between GNN models, while the practicality allows for understanding concrete GNN abilities such as subgraph counting. By examining four classes of prominent GNNs as case studies, we derive simple, unified, and elegant descriptions of their homomorphism expressivity for both invariant and equivariant settings. Our results provide novel insights into a series of previous work, unify the landscape of different subareas in the community, and settle several open questions. Empirically, extensive experiments on both synthetic and real-world tasks verify our theory, showing that the practical performance of GNN models aligns well with the proposed metric.",Oral 2D,https://openreview.net/pdf?id=HSKaGOi7Ar,https://openreview.net/forum?id=HSKaGOi7Ar
"['Yixiao Li', 'Yifan Yu', 'Chen Liang', 'Nikos Karampatziakis', 'Pengcheng He', 'Weizhu Chen', 'Tuo Zhao']",ICLR,LoftQ: LoRA-Fine-Tuning-aware Quantization for Large Language Models,https://iclr.cc/virtual/2024/oral/19765,2024," Quantization is an indispensable technique for serving Large Language Models (LLMs) and has recently found its way into LoRA fine-tuning (Dettmers et al., 2023). In this work we focus on the scenario where quantization and LoRA fine- tuning are applied together on a pre-trained model. In such cases it is common to observe a consistent gap in the performance on downstream tasks between full fine-tuning and quantization plus LoRA fine-tuning approach. In response, we propose LoftQ (LoRA-Fine-Tuning-aware Quantization), a novel quantization framework that simultaneously quantizes an LLM and finds a proper low-rank initialization for LoRA fine-tuning. Such an initialization alleviates the discrep- ancy between the quantized and full-precision model and significantly improves the generalization in downstream tasks. We evaluate our method on natural lan- guage understanding, question answering, summarization, and natural language generation tasks. Experiments show that our method is highly effective and out- performs existing quantization methods, especially in the challenging 2-bit and 2/4-bit mixed precision regimes. We will release our code.",Oral 3A,https://openreview.net/pdf?id=LzPWWPAdY4,https://openreview.net/forum?id=LzPWWPAdY4
"['Yuxuan Song', 'Jingjing Gong', 'Hao Zhou', 'Mingyue Zheng', 'Jingjing Liu', 'Wei-Ying Ma']",ICLR,Unified Generative Modeling of 3D Molecules with Bayesian Flow Networks,https://iclr.cc/virtual/2024/oral/19764,2024," Advanced generative model (\textit{e.g.}, diffusion model) derived from simplified continuity assumptions of data distribution, though showing promising progress, has been difficult to apply directly to geometry generation applications due to the \textit{multi-modality} and \textit{noise-sensitive} nature of molecule geometry. This work introduces Geometric Bayesian Flow Networks (GeoBFN), which naturally fits molecule geometry by modeling diverse modalities in the differentiable parameter space of distributions. GeoBFN maintains the SE-(3) invariant density modeling property by incorporating equivariant inter-dependency modeling on parameters of distributions and unifying the probabilistic modeling of different modalities. Through optimized training and sampling techniques, we demonstrate that GeoBFN achieves state-of-the-art performance on multiple 3D molecule generation benchmarks in terms of generation quality (90.87\% molecule stability in QM9 and 85.6\% atom stability in GEOM-DRUG\footnote{The scores are reported at 1k sampling steps for fair comparison, and our scores could be further improved if sampling sufficiently longer steps.}). GeoBFN can also conduct sampling with any number of steps to reach an optimal trade-off between efficiency and quality (\textit{e.g.}, 20$\times$ speedup without sacrificing performance).",Oral 3B,https://openreview.net/pdf?id=NSVtmmzeRB,https://openreview.net/forum?id=NSVtmmzeRB
"['Bo Zhao', 'Robert M. Gower', 'Robin Walters', 'Rose Yu']",ICLR,Improving Convergence and Generalization Using Parameter Symmetries,https://iclr.cc/virtual/2024/oral/19767,2024," In many neural networks, different values of the parameters may result in the same loss value. Parameter space symmetries are loss-invariant transformations that change the model parameters. Teleportation applies such transformations to accelerate optimization. However, the exact mechanism behind this algorithm's success is not well understood. In this paper, we show that teleportation not only speeds up optimization in the short-term, but gives overall faster time to convergence. Additionally, teleporting to minima with different curvatures improves generalization, which suggests a connection between the curvature of the minimum and generalization ability. Finally, we show that integrating teleportation into a wide range of optimization algorithms and optimization-based meta-learning improves convergence. Our results showcase the versatility of teleportation and demonstrate the potential of incorporating symmetry in optimization.",Oral 3C,https://openreview.net/pdf?id=L0r0GphlIL,https://openreview.net/forum?id=L0r0GphlIL
"['Wenxuan Li', 'Alan Yuille', 'Zongwei Zhou']",ICLR,How Well Do Supervised 3D Models Transfer to Medical Imaging Tasks?,https://iclr.cc/virtual/2024/oral/19781,2024," The pre-training and fine-tuning paradigm has become prominent in transfer learning. For example, if the model is pre-trained on ImageNet and then fine-tuned to PASCAL, it can significantly outperform that trained on PASCAL from scratch. While ImageNet pre-training has shown enormous success, it is formed in 2D, and the learned features are for classification tasks; when transferring to more diverse tasks, like 3D image segmentation, its performance is inevitably compromised due to the deviation from the original ImageNet context. A significant challenge lies in the lack of large, annotated 3D datasets rivaling the scale of ImageNet for model pre-training. To overcome this challenge, we make two contributions. Firstly, we construct AbdomenAtlas 1.1 that comprises 9,262 three-dimensional computed tomography (CT) volumes with high-quality, per-voxel annotations of 25 anatomical structures and pseudo annotations of seven tumor types. Secondly, we develop a suite of models that are pre-trained on our AbdomenAtlas 1.1 for transfer learning. Our preliminary analyses indicate that the model trained only with 21 CT volumes, 672 masks, and 40 GPU hours has a transfer learning ability similar to the model trained with 5,050 (unlabeled) CT volumes and 1,152 GPU hours. More importantly, the transfer learning ability of supervised models can further scale up with larger annotated datasets, achieving significantly better performance than preexisting pre-trained models, irrespective of their pre-training methodologies or data sources. We hope this study can facilitate collective efforts in constructing larger 3D medical datasets and more releases of supervised pre-trained models.",Oral 3D,https://openreview.net/pdf?id=AhizIPytk4,https://openreview.net/forum?id=AhizIPytk4
"['Ismail Akhalwaya', 'Shashanka Ubaru', 'Kenneth Clarkson', 'Mark Squillante', 'Vishnu Jejjala', 'Yang-Hui He', 'Kugendran Naidoo', 'Vasileios Kalantzis', 'Lior Horesh']",ICLR,Topological data analysis on noisy quantum computers,https://iclr.cc/virtual/2024/oral/19742,2024," Topological data analysis (TDA) is a powerful technique for extracting complex and valuable shape-related summaries of high-dimensional data. However, the computational demands of classical algorithms for computing TDA are exorbitant, and quickly become impractical for high-order characteristics. Quantum computers offer the potential of achieving significant speedup for certain computational problems. Indeed, TDA has been purported to be one such problem, yet, quantum computing algorithms proposed for the problem, such as the original Quantum TDA (QTDA) formulation by Lloyd, Garnerone and Zanardi, require fault-tolerance qualifications that are currently unavailable. In this study, we present NISQ-TDA, a fully implemented end-to-end quantum machine learning algorithm needing only a short circuit-depth, that is applicable to high-dimensional classical data, and with provable asymptotic speedup for certain classes of problems. The algorithm neither suffers from the data-loading problem nor does it need to store the input data on the quantum computer explicitly. The algorithm was successfully executed on quantum computing devices, as well as on noisy quantum simulators, applied to small datasets. Preliminary empirical results suggest that the algorithm is robust to noise.",Oral 4B,https://openreview.net/pdf?id=dLrhRIMVmB,https://openreview.net/forum?id=dLrhRIMVmB
"['Carlos E Jimenez', 'John Yang', 'Alexander Wettig', 'Shunyu Yao', 'Kexin Pei', 'Ofir Press', 'Karthik Narasimhan']",ICLR,SWE-bench: Can Language Models Resolve Real-world Github Issues?,https://iclr.cc/virtual/2024/oral/19757,2024," Language models have outpaced our ability to evaluate them effectively, but for their future development it is essential to study the frontier of their capabilities. We find real-world software engineering to be a rich, sustainable, and challenging testbed for evaluating the next generation of language models. To this end, we introduce SWE-bench, an evaluation framework consisting of 2,294 software engineering problems drawn from real GitHub issues and corresponding pull requests across 12 popular Python repositories. Given a codebase along with a description of an issue to be resolved, a language model is tasked with editing the codebase to address the issue. Resolving issues in SWE-bench frequently requires understanding and coordinating changes across multiple functions, classes, and even files simultaneously, calling for models to interact with execution environments, process extremely long contexts and perform complex reasoning that goes far beyond traditional code generation tasks. Our evaluations show that both state-of-the-art proprietary models and our fine-tuned model SWE-Llama can resolve only the simplest issues. The best-performing model, Claude 2, is able to solve a mere 1.96% of the issues. Advances on SWE-bench represent steps towards LMs that are more practical, intelligent, and autonomous.",Oral 4A,https://openreview.net/pdf?id=VTF8yNQM66,https://openreview.net/forum?id=VTF8yNQM66
"['Hyungho Na', 'Yunkyeong Seo', 'Il-chul Moon']",ICLR,Efficient Episodic Memory Utilization of Cooperative Multi-Agent Reinforcement Learning,https://iclr.cc/virtual/2024/oral/19766,2024," In cooperative multi-agent reinforcement learning (MARL), agents aim to achieve a common goal, such as defeating enemies or scoring a goal. Existing MARL algorithms are effective but still require significant learning time and often get trapped in local optima by complex tasks, subsequently failing to discover a goal-reaching policy. To address this, we introduce Efficient episodic Memory Utilization (EMU) for MARL, with two primary objectives: (a) accelerating reinforcement learning by leveraging semantically coherent memory from an episodic buffer and (b) selectively promoting desirable transitions to prevent local convergence. To achieve (a), EMU incorporates a trainable encoder/decoder structure alongside MARL, creating coherent memory embeddings that facilitate exploratory memory recall. To achieve (b), EMU introduces a novel reward structure called episodic incentive based on the desirability of states. This reward improves the TD target in Q-learning and acts as an additional incentive for desirable transitions. We provide theoretical support for the proposed incentive and demonstrate the effectiveness of EMU compared to conventional episodic control. The proposed method is evaluated in StarCraft II and Google Research Football, and empirical results indicate further performance improvement over state-of-the-art methods.",Oral 4C,https://openreview.net/pdf?id=LjivA1SLZ6,https://openreview.net/forum?id=LjivA1SLZ6
"['Edward Hu', 'Moksh Jain', 'Eric Elmoznino', 'Younesse Kaddar', 'Guillaume Lajoie', 'Yoshua Bengio', 'Nikolay Malkin']",ICLR,Amortizing intractable inference in large language models,https://iclr.cc/virtual/2024/oral/19763,2024," Autoregressive large language models (LLMs) compress knowledge from their training data through next-token conditional distributions. This limits tractable querying of this knowledge to start-to-end autoregressive sampling. However, many tasks of interest---including sequence continuation, infilling, and other forms of constrained generation---involve sampling from intractable posterior distributions. We address this limitation by using amortized Bayesian inference to sample from these intractable posteriors. Such amortization is algorithmically achieved by fine-tuning LLMs via diversity-seeking reinforcement learning algorithms: generative flow networks (GFlowNets). We empirically demonstrate that this distribution-matching paradigm of LLM fine-tuning can serve as an effective alternative to maximum-likelihood training and reward-maximizing policy optimization. As an important application, we interpret chain-of-thought reasoning as a latent variable modeling problem and demonstrate that our approach enables data-efficient adaptation of LLMs to tasks that require multi-step rationalization and tool use.",Oral 4D,https://openreview.net/pdf?id=Ouj6p4ca60,https://openreview.net/forum?id=Ouj6p4ca60
"['Zahra Kadkhodaie', 'Florentin Guth', 'Eero Simoncelli', 'Stéphane Mallat']",ICLR,Generalization in diffusion models arises from geometry-adaptive harmonic representations,https://iclr.cc/virtual/2024/oral/19783,2024," Deep neural networks (DNNs) trained for image denoising are able to generate high-quality samples with score-based reverse diffusion algorithms. These impressive capabilities seem to imply an escape from the curse of dimensionality, but recent reports of memorization of the training set raise the question of whether these networks are learning the ""true"" continuous density of the data. Here, we show that two DNNs trained on non-overlapping subsets of a dataset learn nearly the same score function, and thus the same density, when the number of training images is large enough.  In this regime of strong generalization, diffusion-generated images are distinct from the training set, and are of high visual quality, suggesting that the inductive biases of the DNNs are well-aligned with the data density. We analyze the learned denoising functions and show that the inductive biases give rise to a shrinkage operation in a basis adapted to the underlying image. Examination of these bases reveals oscillating harmonic structures along contours and in homogeneous regions. We demonstrate that trained denoisers are inductively biased towards these geometry-adaptive harmonic bases since they arise not only when the network is trained on photographic images, but also when it is trained on image classes supported on low-dimensional manifolds for which the harmonic basis is suboptimal. Finally, we show that when trained on regular image classes for which the optimal basis is known to be geometry-adaptive and harmonic, the denoising performance of the networks is near-optimal.",Oral 5A,https://openreview.net/pdf?id=ANvmVS2Yr0,https://openreview.net/forum?id=ANvmVS2Yr0
"['Xiangyu Qi', 'Yi Zeng', 'Tinghao Xie', 'Pin-Yu Chen', 'Ruoxi Jia', 'Prateek Mittal', 'Peter Henderson']",ICLR,"Fine-tuning Aligned Language Models Compromises Safety, Even When Users Do Not Intend To!",https://iclr.cc/virtual/2024/oral/19735,2024," Optimizing large language models (LLMs) for downstream use cases often involves the customization of pre-trained LLMs through further fine-tuning. Meta's open-source release of Llama models and OpenAI's APIs for fine-tuning GPT-3.5 Turbo on customized datasets accelerate this trend. But, what are the safety costs associated with such customized fine-tuning? While existing safety alignment techniques restrict harmful behaviors of LLMs at inference time, they do not cover safety risks when fine-tuning privileges are extended to end-users. Our red teaming studies find that the safety alignment of LLMs can be compromised by fine-tuning with only a few adversarially designed training examples. For instance, we jailbreak GPT-3.5 Turbo's safety guardrails by fine-tuning it on only 10 such examples at a cost of less than $0.20 via OpenAI's APIs, making the model responsive to nearly any harmful instructions. Disconcertingly, our research also reveals that, even without malicious intent, simply fine-tuning with benign and commonly used datasets can also inadvertently degrade the safety alignment of LLMs, though to a lesser extent. These findings suggest that fine-tuning aligned LLMs introduces new safety risks that current safety infrastructures fall short of addressing --- even if a model's initial safety alignment is impeccable, how can it be maintained after customized fine-tuning? We outline and critically analyze potential mitigations and advocate for further research efforts toward reinforcing safety protocols for the customized fine-tuning of aligned LLMs.  (This paper contains red-teaming data and model-generated content that can be offensive in nature.)",Oral 5B,https://openreview.net/pdf?id=hTEGyKf0dZ,https://openreview.net/forum?id=hTEGyKf0dZ
"['Panagiotis Eustratiadis', 'Łukasz Dudziak', 'Da Li', 'Timothy Hospedales']",ICLR,Neural Fine-Tuning Search for Few-Shot Learning,https://iclr.cc/virtual/2024/oral/19760,2024," In few-shot recognition, a classifier that has been trained on one set of classes is required to rapidly adapt and generalize to a disjoint, novel set of classes. To that end, recent studies have shown the efficacy of fine-tuning with carefully-crafted adaptation architectures. However this raises the question of: How can one design the optimal adaptation strategy? In this paper, we study this question through the lens of neural architecture search (NAS). Given a pre-trained neural network, our algorithm discovers the optimal arrangement of adapters, which layers to keep frozen, and which to fine-tune. We demonstrate the generality of our NAS method by applying it to both residual networks and vision transformers and report state-of-the-art performance on Meta-Dataset and Meta-Album.",Oral 5C,https://openreview.net/pdf?id=T7YV5UZKBc,https://openreview.net/forum?id=T7YV5UZKBc
"['Yossi Gandelsman', 'Alexei Efros', 'Jacob Steinhardt']",ICLR,Interpreting CLIP's Image Representation via Text-Based Decomposition,https://iclr.cc/virtual/2024/oral/19791,2024," We investigate the CLIP image encoder by analyzing how individual model components affect the final representation. We decompose the image representation as a sum across individual image patches, model layers, and attention heads, and use CLIP's text representation to interpret the summands. Interpreting the attention heads, we characterize each head's role by automatically finding text representations that span its output space, which reveals property-specific roles for many heads (e.g. location or shape). Next, interpreting the image patches, we uncover an emergent spatial localization within CLIP. Finally, we use this understanding to remove spurious features from CLIP and to create a strong zero-shot image segmenter. Our results indicate that scalable understanding of transformer models is attainable and can be used to repair and improve models.",Oral 5D,https://openreview.net/pdf?id=5Ca9sSzuDp,https://openreview.net/forum?id=5Ca9sSzuDp
"['Giorgio Mariani', 'Irene Tallini', 'Emilian Postolache', 'Michele Mancusi', 'Luca Cosmo', 'Emanuele Rodolà']",ICLR,Multi-Source Diffusion Models for Simultaneous Music Generation and Separation,https://iclr.cc/virtual/2024/oral/19737,2024," In this work, we define a diffusion-based generative model capable of both music generation and source separation by learning the score of the joint probability density of sources sharing a context. Alongside the classic total inference tasks (i.e., generating a mixture, separating the sources), we also introduce and experiment on the partial generation task of source imputation, where we generate a subset of the sources given the others (e.g., play a piano track that goes well with the drums). Additionally, we introduce a novel inference method for the separation task based on Dirac likelihood functions. We train our model on Slakh2100, a standard dataset for musical source separation, provide qualitative results in the generation settings, and showcase competitive quantitative results in the source separation setting. Our method is the first example of a single model that can handle both generation and separation tasks, thus representing a step toward general audio models.",Oral 6A,https://openreview.net/pdf?id=h922Qhkmx1,https://openreview.net/forum?id=h922Qhkmx1
"['Shuo He', 'Chaojie Wang', 'Guowu Yang', 'Lei Feng']",ICLR,Candidate Label Set Pruning: A Data-centric Perspective for Deep Partial-label Learning,https://iclr.cc/virtual/2024/oral/19776,2024," Partial-label learning (PLL) allows each training example to be equipped with a set of candidate labels. Existing deep PLL research focuses on a \emph{learning-centric} perspective to design various training strategies for label disambiguation i.e., identifying the concealed true label from the candidate label set, for model training. However, when the size of the candidate label set becomes excessively large, these learning-centric strategies would be unable to find the true label for model training, thereby causing performance degradation. This motivates us to think from a \emph{data-centric} perspective and pioneer a new PLL-related task called candidate label set pruning (CLSP) that aims to filter out certain potential false candidate labels in a training-free manner. To this end, we propose the first CLSP method based on the inconsistency between the representation space and the candidate label space. Specifically, for each candidate label of a training instance, if it is not a candidate label of the instance's nearest neighbors in the representation space, then it has a high probability of being a false label. Based on this intuition, we employ a per-example pruning scheme that filters out a specific proportion of high-probability false candidate labels. Theoretically, we prove an upper bound of the pruning error rate and analyze how the quality of representations affects our proposed method. Empirically, extensive experiments on both benchmark-simulated and real-world PLL datasets validate the great value of CLSP to significantly improve many state-of-the-art deep PLL methods.",Oral 6C,https://openreview.net/pdf?id=Fk5IzauJ7F,https://openreview.net/forum?id=Fk5IzauJ7F
"['Zaishuo Xia', 'Han Yang', 'Binghui Wang', 'Jinyuan Jia']",ICLR,GNNCert: Deterministic Certification of Graph Neural Networks against Adversarial Perturbations,https://iclr.cc/virtual/2024/oral/19771,2024," Graph classification, which aims to predict a label for a graph, has many real-world applications such as malware detection, fraud detection, and healthcare. However, many studies show an attacker could carefully perturb the structure and/or node features in a graph such that a graph classifier misclassifies the perturbed graph. Such vulnerability impedes the deployment of graph classification in security/safety-critical applications. Existing empirical defenses lack formal robustness guarantees and could be broken by adaptive or unknown attacks. Existing provable defenses have the following limitations: 1)  they achieve sub-optimal robustness guarantees for graph structure perturbation, 2) they cannot provide robustness guarantees for arbitrarily node feature perturbations, 3) their robustness guarantees are probabilistic, meaning they could be incorrect with a non-zero probability, and 4) they incur large computation costs. We aim to address those limitations in this work. We propose GNNCert, a certified defense against both graph structure and node feature perturbations for graph classification. Our GNNCert provably predicts the same label for a graph when the number of perturbed edges and the number of nodes with perturbed features are bounded. Our results on 8 benchmark datasets show that GNNCert outperforms three state-of-the-art methods.",Oral 6B,https://openreview.net/pdf?id=IGzaH538fz,https://openreview.net/forum?id=IGzaH538fz
"['Yijie Lin', 'Jie Zhang', 'Zhenyu Huang', 'Jia Liu', 'zujie wen', 'Xi Peng']",ICLR,Multi-granularity Correspondence Learning from Long-term Noisy Videos,https://iclr.cc/virtual/2024/oral/19786,2024," Existing video-language studies mainly focus on learning short video clips, leaving long-term temporal dependencies rarely explored due to over-high computational cost of modeling long videos. To address this issue, one feasible solution is learning the correspondence between video clips and captions, which however inevitably encounters the multi-granularity noisy correspondence (MNC) problem. To be specific, MNC refers to the clip-caption misalignment (coarse-grained) and frame-word misalignment (fine-grained), hindering temporal learning and video understanding. In this paper, we propose NOise Robust Temporal Optimal traNsport (Norton) that addresses MNC in a unified optimal transport (OT) framework. In brief, Norton employs video-paragraph and clip-caption contrastive losses to capture long-term dependencies based on OT. To address coarse-grained misalignment in video-paragraph contrast, Norton filters out the irrelevant clips and captions through an alignable prompt bucket and realigns asynchronous clip-caption pairs based on transport distance. To address the fine-grained misalignment, Norton incorporates a soft-maximum operator to identify crucial words and key frames. Additionally, Norton exploits the potential faulty negative samples in clip-caption contrast by rectifying the alignment target with OT assignment to ensure precise temporal modeling. Extensive experiments on video retrieval, videoQA, and action segmentation verify the effectiveness of our method. Code is available at https://lin-yijie.github.io/projects/Norton.",Oral 6D,https://openreview.net/pdf?id=9Cu8MRmhq2,https://openreview.net/forum?id=9Cu8MRmhq2
"['Galen Andrew', 'Peter Kairouz', 'Sewoong Oh', 'Alina Oprea', 'H. Brendan McMahan', 'Vinith Suriyakumar']",ICLR,One-shot Empirical Privacy Estimation for Federated Learning,https://iclr.cc/virtual/2024/oral/19797,2024," Privacy estimation techniques for differentially private (DP) algorithms are useful for comparing against analytical bounds, or to empirically measure privacy loss in settings where known analytical bounds are not tight. However, existing privacy auditing techniques usually make strong assumptions on the adversary (e.g., knowledge of intermediate model iterates or the training data distribution), are tailored to specific tasks, model architectures, or DP algorithm, and/or require retraining the model many times (typically on the order of thousands). These shortcomings make deploying such techniques at scale difficult in practice, especially in federated settings where model training can take days or weeks. In this work, we present a novel “one-shot” approach that can systematically address these challenges, allowing efficient auditing or estimation of the privacy loss of a model during the same, single training run used to fit model parameters, and without requiring any a priori knowledge about the model architecture, task, or DP algorithm. We show that our method provides provably correct estimates for the privacy loss under the Gaussian mechanism, and we demonstrate its performance on a well-established FL benchmark dataset under several adversarial threat models.",Oral 7D,https://openreview.net/pdf?id=0BqyZSWfzo,https://openreview.net/forum?id=0BqyZSWfzo
"['Mitchell Wortsman', 'Peter Liu', 'Lechao Xiao', 'Katie Everett', 'Alexander Alemi', 'Ben Adlam', 'John Co-Reyes', 'Izzeddin Gur', 'Abhishek Kumar', 'Roman Novak', 'Jeffrey Pennington', 'Jascha Sohl-Dickstein', 'Kelvin Xu', 'Jaehoon Lee', 'Justin Gilmer', 'Simon Kornblith']",ICLR,Small-scale proxies for large-scale Transformer training instabilities,https://iclr.cc/virtual/2024/oral/19743,2024," Teams that have trained large Transformer-based models have reported training instabilities at large scale that did not appear when training with the same hyperparameters at smaller scales. Although the causes of such instabilities are of scientific interest, the amount of resources required to reproduce them has made investigation difficult. In this work, we seek ways to reproduce and study training instability at smaller scales. First, we focus on two sources of training instability described in previous work: the growth of logits in attention layers (Dehghani et al., 2023) and divergence of the output logits from the log probabilities (Chowdhery et al., 2022). By measuring the relationship between learning rate and loss across scales, we show that these instabilities also appear in small models when training at high learning rates, and that mitigations previously employed at large scales are equally effective in this regime. This prompts us to investigate the extent to which other known optimizer and model interventions influence the sensitivity of the final loss to changes in the learning rate. To this end, we study methods such as warm-up, weight decay, and the MuParam (Yang et al., 2022), and combine techniques to train small models that achieve similar losses across orders of magnitude of learning rate variation. Finally, to conclude our exploration we study two cases where instabilities can be predicted before they emerge by examining the scaling behavior of model characteristics such as activation and gradient norms.",Oral 7A,https://openreview.net/pdf?id=d8w0pmvXbZ,https://openreview.net/forum?id=d8w0pmvXbZ
"['Jiaxiang Tang', 'Jiawei Ren', 'Hang Zhou', 'Ziwei Liu', 'Gang Zeng']",ICLR,DreamGaussian: Generative Gaussian Splatting for Efficient 3D Content Creation,https://iclr.cc/virtual/2024/oral/19758,2024," Recent advances in 3D content creation mostly leverage optimization-based 3D generation via score distillation sampling (SDS).Though promising results have been exhibited, these methods often suffer from slow per-sample optimization, limiting their practical usage. In this paper, we propose DreamGaussian, a novel 3D content generation framework that achieves both efficiency and quality simultaneously. Our key insight is to design a generative 3D Gaussian Splatting model with companioned mesh extraction and texture refinement in UV space.In contrast to the occupancy pruning used in Neural Radiance Fields, we demonstrate that the progressive densification of 3D Gaussians converges significantly faster for 3D generative tasks.To further enhance the texture quality and facilitate downstream applications, we introduce an efficient algorithm to convert 3D Gaussians into textured meshes and apply a fine-tuning stage to refine the details.Extensive experiments demonstrate the superior efficiency and competitive generation quality of our proposed approach.Notably, DreamGaussian produces high-quality textured meshes in just 2 minutes from a single-view image, achieving approximately 10 times acceleration compared to existing methods.",Oral 7B,https://openreview.net/pdf?id=UyNXMqnN3c,https://openreview.net/forum?id=UyNXMqnN3c
"['Ruoyu Chen', 'Hua Zhang', 'Siyuan Liang', 'Jingzhi Li', 'Xiaochun Cao']",ICLR,Less is More: Fewer Interpretable Region via Submodular Subset Selection,https://iclr.cc/virtual/2024/oral/19733,2024," Image attribution algorithms aim to identify important regions that are highly relevant to model decisions. Although existing attribution solutions can effectively assign importance to target elements, they still face the following challenges: 1) existing attribution methods generate inaccurate small regions thus misleading the direction of correct attribution, and 2) the model cannot produce good attribution results for samples with wrong predictions. To address the above challenges, this paper re-models the above image attribution problem as a submodular subset selection problem, aiming to enhance model interpretability using fewer regions. To address the lack of attention to local regions, we construct a novel submodular function to discover more accurate small interpretation regions. To enhance the attribution effect for all samples, we also impose four different constraints on the selection of sub-regions, i.e., confidence, effectiveness, consistency, and collaboration scores, to assess the importance of various subsets. Moreover, our theoretical analysis substantiates that the proposed function is in fact submodular. Extensive experiments show that the proposed method outperforms SOTA methods on two face datasets (Celeb-A and VGG-Face2) and one fine-grained dataset (CUB-200-2011). For correctly predicted samples, the proposed method improves the Deletion and Insertion scores with an average of 4.9\% and 2.5\% gain relative to HSIC-Attribution. For incorrectly predicted samples, our method achieves gains of 81.0\% and 18.4\% compared to the HSIC-Attribution algorithm in the average highest confidence and Insertion score respectively. The code is released at https://github.com/RuoyuChen10/SMDL-Attribution.",Oral 7C,https://openreview.net/pdf?id=jKTUlxo5zy,https://openreview.net/forum?id=jKTUlxo5zy
"['Xian Li', 'Ping Yu', 'Chunting Zhou', 'Timo Schick', 'Omer Levy', 'Luke Zettlemoyer', 'Jason E Weston', 'Mike Lewis']",ICLR,Self-Alignment with Instruction Backtranslation,https://iclr.cc/virtual/2024/oral/19796,2024," We present a scalable method to build a high quality instruction following language model by automatically labelling human-written text with corresponding instructions. Our approach, named instruction backtranslation, starts with a language model finetuned on a small amount of seed data, and a given web corpus. The seed model is used to construct training examples by generating instruction prompts for web documents (self-augmentation), and then  selecting high quality examples from among these candidates (self-curation).  This data is then used to finetune a stronger model.  Finetuning LLaMa on two iterations of our approach yields a model that outperforms all other LLaMa-based models on the Alpaca leaderboard not relying on distillation data, demonstrating highly effective self-alignment.",Oral 8A,https://openreview.net/pdf?id=1oijHJBRsT,https://openreview.net/forum?id=1oijHJBRsT
"['Jie Hu', 'Vishwaraj Doshi', 'Do Young Eun']",ICLR,Accelerating Distributed Stochastic Optimization via Self-Repellent Random Walks,https://iclr.cc/virtual/2024/oral/19780,2024," We study a family of distributed stochastic optimization algorithms where gradients are sampled by a token traversing a network of agents in random-walk fashion. Typically, these random-walks are chosen to be Markov chains that asymptotically sample from a desired target distribution, and play a critical role in the convergence of the optimization iterates. In this paper, we take a novel approach by replacing the standard *linear* Markovian token by one which follows a *non-linear* Markov chain - namely the Self-Repellent Radom Walk (SRRW). Defined for any given 'base' Markov chain, the SRRW, parameterized by a positive scalar $\\alpha$, is less likely to transition to states that were highly visited in the past, thus the name. In the context of MCMC sampling on a graph, a recent breakthrough in Doshi et al. (2023) shows that the SRRW achieves $O(1/\\alpha)$ decrease in the asymptotic variance for sampling. We propose the use of a `generalized' version of the SRRW to drive token algorithms for distributed stochastic optimization in the form of stochastic approximation, termed SA-SRRW. We prove that the optimization iterate errors of the resulting SA-SRRW converge to zero almost surely and prove a central limit theorem, deriving the explicit form of the resulting asymptotic covariance matrix corresponding to iterate errors. This asymptotic covariance is always smaller than that of an algorithm driven by the base Markov chain and decreases at rate $O(1/\\alpha^2)$ - the performance benefit of using SRRW thereby *amplified* in the stochastic optimization context. Empirical results support our theoretical findings.",Oral 8B,https://openreview.net/pdf?id=BV1PHbTJzd,https://openreview.net/forum?id=BV1PHbTJzd
"['Mohammad Reza Samsami', 'Artem Zholus', 'Janarthanan Rajendran', 'Sarath Chandar']",ICLR,Mastering Memory Tasks with World Models,https://iclr.cc/virtual/2024/oral/19795,2024," Current model-based reinforcement learning (MBRL) agents struggle with long-term dependencies. This limits their ability to effectively solve tasks involving extended time gaps between actions and outcomes, or tasks demanding the recalling of distant observations to inform current actions. To improve temporal coherence, we integrate a new family of state space models (SSMs) in world models of MBRL agents to present a new method, Recall to Imagine (R2I). This integration aims to enhance both long-term memory and long-horizon credit assignment. Through a diverse set of illustrative tasks, we systematically demonstrate that R2I not only establishes a new state-of-the-art for challenging memory and credit assignment RL tasks, such as BSuite and POPGym, but also showcases superhuman performance in the complex memory domain of Memory Maze. At the same time, it upholds comparable performance in classic RL tasks, such as Atari and DMC, suggesting the generality of our method. We also show that R2I is faster than the state-of-the-art MBRL method, DreamerV3, resulting in faster wall-time convergence.",Oral 8C,https://openreview.net/pdf?id=1vDArHJ68h,https://openreview.net/forum?id=1vDArHJ68h
"['Thaddäus Wiedemer', 'Jack Brady', 'Alexander Panfilov', 'Attila Juhos', 'Matthias Bethge', 'Wieland Brendel']",ICLR,Provable Compositional Generalization for Object-Centric Learning,https://iclr.cc/virtual/2024/oral/19788,2024," Learning representations that generalize to novel compositions of known concepts is crucial for bridging the gap between human and machine perception. One prominent effort is learning object-centric representations, which are widely conjectured to enable compositional generalization. Yet, it remains unclear when this conjecture will be true, as a principled theoretical or empirical understanding of compositional generalization is lacking. In this work, we investigate when compositional generalization is guaranteed for object-centric representations through the lens of identifiability theory. We show that autoencoders that satisfy structural assumptions on the decoder and enforce encoder-decoder consistency will learn object-centric representations that provably generalize compositionally. We validate our theoretical result and highlight the practical relevance of our assumptions through experiments on synthetic image data.",Oral 8D,https://openreview.net/pdf?id=7VPTUWkiDQ,https://openreview.net/forum?id=7VPTUWkiDQ
"['Yogesh Verma', 'Markus Heinonen', 'Vikas Garg']",ICLR,ClimODE: Climate and Weather Forecasting with Physics-informed Neural ODEs,https://iclr.cc/virtual/2024/oral/19715,2024," Climate and weather prediction traditionally relies on complex numerical simulations of atmospheric physics. Deep learning approaches, such as transformers, have recently challenged the simulation paradigm with complex network forecasts. However, they often act as data-driven black-box models that neglect the underlying physics and lack uncertainty quantification. We address these limitations with ClimODE, a  spatiotemporal continuous-time process that implements a key principle of advection from statistical mechanics, namely, weather changes due to a spatial movement of quantities over time. ClimODE models precise weather evolution with value-conserving dynamics, learning global weather transport as a neural flow, which also enables estimating the uncertainty in predictions. Our approach outperforms existing data-driven methods in global and regional forecasting with an order of magnitude smaller parameterization, establishing a new state of the art.",Oral 1A,https://openreview.net/pdf?id=xuY33XhEGR,https://openreview.net/forum?id=xuY33XhEGR
"['Jonathan Richens', 'Tom Everitt']",ICLR,Robust agents learn causal world models,https://iclr.cc/virtual/2024/oral/19724,2024," It has long been hypothesised that causal reasoning plays a fundamental role in robust and general intelligence. However, it is not known if agents must learn causal models in order to generalise to new domains, or if other inductive biases are sufficient. We answer this question, showing that any agent capable of satisfying a regret bound for a large set of distributional shifts must have learned an approximate causal model of the data generating process, which converges to the true causal model for optimal agents. We discuss the implications of this result for several research areas including transfer learning and causal inference.",Oral 1D,https://openreview.net/pdf?id=pOoKI3ouv1,https://openreview.net/forum?id=pOoKI3ouv1
"['Marius Memmel', 'Andrew Wagenmaker', 'Chuning Zhu', 'Dieter Fox', 'Abhishek Gupta']",ICLR,ASID: Active Exploration for System Identification in Robotic Manipulation,https://iclr.cc/virtual/2024/oral/19732,2024," Model-free control strategies such as reinforcement learning have shown the ability to learn control strategies without requiring an accurate model or simulator of the world. While this is appealing due to the lack of modeling requirements, such methods can be sample inefficient, making them impractical in many real-world domains. On the other hand, model-based control techniques leveraging accurate simulators can circumvent these challenges and use a large amount of cheap simulation data to learn controllers that can effectively transfer to the real world. The challenge with such model-based techniques is the requirement for an extremely accurate simulation, requiring both the specification of appropriate simulation assets and physical parameters. This requires considerable human effort to design for every environment being considered. In this work, we propose a learning system that can leverage a small amount of real-world data to autonomously refine a simulation model and then plan an accurate control strategy that can be deployed in the real world. Our approach critically relies on utilizing an initial (possibly inaccurate) simulator to design effective exploration policies that, when deployed in the real world, collect high-quality data. We demonstrate the efficacy of this paradigm in identifying articulation, mass, and other physical parameters in several challenging robotic manipulation tasks, and illustrate that only a small amount of real-world data can allow for effective sim-to-real transfer.",Oral 1B,https://openreview.net/pdf?id=jNR6s6OSBT,https://openreview.net/forum?id=jNR6s6OSBT
"['Pan Lu', 'Hritik Bansal', 'Tony Xia', 'Jiacheng Liu', 'Chunyuan Li', 'Hannaneh Hajishirzi', 'Hao Cheng', 'Kai-Wei Chang', 'Michel Galley', 'Jianfeng Gao']",ICLR,MathVista: Evaluating Mathematical Reasoning of Foundation Models in Visual Contexts,https://iclr.cc/virtual/2024/oral/19768,2024," Large Language Models (LLMs) and Large Multimodal Models (LMMs) exhibit impressive problem-solving skills in many tasks and domains, but their ability in mathematical reasoning in visual contexts has not been systematically studied. To bridge this gap, we present MathVista, a benchmark designed to combine challenges from diverse mathematical and visual tasks. It consists of 6,141 examples, derived from 28 existing multimodal datasets involving mathematics and 3 newly created datasets (i.e., IQTest, FunctionQA, and PaperQA). Completing these tasks requires fine-grained, deep visual understanding and compositional reasoning, which all state-of-the-art foundation models find challenging. With MathVista, we have conducted a comprehensive, quantitative evaluation of 12 prominent foundation models. The best-performing GPT-4V model achieves an overall accuracy of 49.9%, substantially outperforming Bard, the second-best performer, by 15.1%. Our in-depth analysis reveals that the superiority of GPT-4V is mainly attributed to its enhanced visual perception and mathematical reasoning. However, GPT-4V still falls short of human performance by 10.4%, as it often struggles to understand complex figures and perform rigorous reasoning. This significant gap underscores the critical role that MathVista will play in the development of general-purpose AI agents capable of tackling mathematically intensive and visually rich real-world tasks. We further explore the new ability of self-verification, the application of self-consistency, and the interactive chatbot capabilities of GPT-4V, highlighting its promising potential for future research. The project is available at https://mathvista.github.io/.",Oral 1C,https://openreview.net/pdf?id=KUNzEQMWU7,https://openreview.net/forum?id=KUNzEQMWU7
"['Zhen Liu', 'Yao Feng', 'Yuliang Xiu', 'Weiyang Liu', 'Liam Paull', 'Michael J Black', 'Bernhard Schoelkopf']",ICLR,Ghost on the Shell: An Expressive Representation of General 3D Shapes,https://iclr.cc/virtual/2024/oral/19782,2024," The creation of photorealistic virtual worlds requires the accurate modeling of 3D surface geometry for a wide range of objects. For this, meshes are appealing since they enable 1) fast physics-based rendering with realistic material and lighting, 2) physical simulation, and 3) are memory-efficient for modern graphics pipelines. Recent work on reconstructing and statistically modeling 3D shape, however, has critiqued meshes as being topologically inflexible. To capture a wide range of object shapes, any 3D representation must be able to model solid, watertight, shapes as well as thin, open, surfaces. Recent work has focused on the former, and methods for reconstructing open surfaces do not support fast reconstruction with material and lighting or unconditional generative modelling. Inspired by the observation that open surfaces can be seen as islands floating on watertight surfaces, we parametrize open surfaces by defining a manifold signed distance field on watertight templates. With this parametrization, we further develop a grid-based and differentiable representation that parametrizes both watertight and non-watertight meshes of arbitrary topology. Our new representation, called Ghost-on-the-Shell (G-Shell), enables two important applications:  differentiable rasterization-based reconstruction from multiview images and generative modelling of non-watertight meshes. We empirically demonstrate that G-Shell achieves state-of-the-art performance on non-watertight mesh reconstruction and generation tasks, while also performing effectively for watertight meshes.",Oral 2B,https://openreview.net/pdf?id=Ad87VjRqUw,https://openreview.net/forum?id=Ad87VjRqUw
"['Satwik Bhattamishra', 'Arkil Patel', 'Phil Blunsom', 'Varun Kanade']",ICLR,Understanding In-Context Learning in Transformers and LLMs by Learning to Learn Discrete Functions,https://iclr.cc/virtual/2024/oral/19741,2024," In order to understand the in-context learning phenomenon, recent works have adopted a stylized experimental framework and demonstrated that Transformers can match the performance of gradient-based learning algorithms for various classes of real-valued functions. However, the limitations of Transformers in implementing learning algorithms, and their ability to learn other forms of algorithms are not well understood. Additionally, the degree to which these capabilities are confined to attention-based models is unclear. Furthermore, it remains to be seen whether the insights derived from these stylized settings can be extrapolated to pretrained Large Language Models (LLMs). In this work, we take a step towards answering these questions by demonstrating the following: (a) On a test-bed with a variety of Boolean function classes, we find that Transformers can nearly match the optimal learning algorithm for 'simpler' tasks, while their performance deteriorates on more 'complex' tasks. Additionally, we find that certain attention-free models perform (almost) identically to Transformers on a range of tasks. (b) When provided a teaching sequence , i.e. a set of examples that uniquely identifies a function in a class, we show that Transformers learn more sample-efficiently. Interestingly, our results show that Transformers can learn to implement two distinct algorithms to solve a single task, and can adaptively select the more sample-efficient algorithm depending on the sequence of in-context examples. (c) Lastly, we show that extant LLMs, e.g. LLaMA-2, GPT-4, can compete with nearest-neighbor baselines on prediction tasks that are guaranteed to not be in their training set.",Oral 2A,https://openreview.net/pdf?id=ekeyCgeRfC,https://openreview.net/forum?id=ekeyCgeRfC
"['Yang Song', 'Prafulla Dhariwal']",ICLR,Improved Techniques for Training Consistency Models,https://iclr.cc/virtual/2024/oral/19754,2024," Consistency models are a nascent family of generative models that can sample high quality data in one step without the need for adversarial training. Current consistency models achieve optimal sample quality by distilling from pre-trained diffusion models and employing learned metrics such as LPIPS. However, distillation limits the quality of consistency models to that of the pre-trained diffusion model, and LPIPS causes undesirable bias in evaluation. To tackle these challenges, we present improved techniques for consistency training, where consistency models learn directly from data without distillation. We delve into the theory behind consistency training and identify a previously overlooked flaw, which we address by eliminating Exponential Moving Average from the teacher consistency model. To replace learned metrics like LPIPS, we adopt Pseudo-Huber losses from robust statistics. Additionally, we introduce a lognormal noise schedule for the consistency training objective, and propose to double total discretization steps every set number of training iterations. Combined with better hyperparameter tuning, these modifications enable consistency models to achieve FID scores of 2.51 and 3.25 on CIFAR-10 and ImageNet $64\times 64$ respectively in a single sampling step. These scores mark a 3.5$\times$ and 4$\times$ improvement compared to prior consistency training approaches. Through two-step sampling, we further reduce FID scores to 2.24 and 2.77 on these two datasets, surpassing those obtained via distillation in both one-step and two-step settings, while narrowing the gap between consistency models and other state-of-the-art generative models.",Oral 2C,https://openreview.net/pdf?id=WNzy9bRDvG,https://openreview.net/forum?id=WNzy9bRDvG
['Gautam Reddy Nallamala'],ICLR,The mechanistic basis of data dependence and abrupt learning in an in-context classification task,https://iclr.cc/virtual/2024/oral/19749,2024," Transformer models exhibit in-context learning: the ability to accurately predict the response to a novel query based on illustrative examples in the input sequence, which contrasts with traditional in-weights learning of query-output relationships. What aspects of the training data distribution and architecture favor in-context vs in-weights learning? Recent work has shown that specific distributional properties inherent in language, such as burstiness, large dictionaries and skewed rank-frequency distributions, control the trade-off or simultaneous appearance of these two forms of learning. We first show that these results are recapitulated in a minimal attention-only network trained on a simplified dataset. In-context learning (ICL) is driven by the abrupt emergence of an induction head, which subsequently competes with in-weights learning. By identifying progress measures that precede in-context learning and targeted experiments, we construct a two-parameter model of an induction head which emulates the full data distributional dependencies displayed by the attention-based network. A phenomenological model of induction head formation traces its abrupt emergence to the sequential learning of three nested logits enabled by an intrinsic curriculum. We propose that the sharp transitions in attention-based networks arise due to a specific chain of multi-layer operations necessary to achieve ICL, which is implemented by nested nonlinearities sequentially learned during training.",Oral 2D,https://openreview.net/pdf?id=aN4Jf6Cx69,https://openreview.net/forum?id=aN4Jf6Cx69
"['Linlu Qiu', 'Liwei Jiang', 'Ximing Lu', 'Melanie Sclar', 'Valentina Pyatkin', 'Chandra Bhagavatula', 'Bailin Wang', 'Yoon Kim', 'Yejin Choi', 'Nouha Dziri', 'Xiang Ren']",ICLR,Phenomenal Yet Puzzling: Testing Inductive Reasoning Capabilities of Language Models with Hypothesis Refinement,https://iclr.cc/virtual/2024/oral/19747,2024," The ability to derive underlying principles from a handful of observations and then generalize to novel situations---known as inductive reasoning---is central to human intelligence. Prior work suggests that language models (LMs) often fall short on inductive reasoning, despite achieving impressive success on research benchmarks. In this work, we conduct a systematic study of the inductive reasoning capabilities of LMs through $\textit{iterative hypothesis refinement}$, a technique that more closely mirrors the human inductive process than standard input-output prompting. Iterative hypothesis refinement employs a three-step process: proposing, selecting, and refining hypotheses in the form of textual rules. By examining the intermediate rules, we observe that LMs are phenomenal $\textit{hypothesis proposers}$ (i.e., generating candidate rules), and when coupled with a (task-specific) symbolic interpreter that is able to systematically filter the proposed set of rules, this hybrid approach achieves strong results across inductive reasoning benchmarks that require inducing causal relations, language-like instructions, and symbolic concepts. However, they also behave as puzzling $\textit{inductive reasoners}$, showing notable performance gaps between rule induction (i.e., identifying plausible rules) and rule application (i.e., applying proposed rules to instances), suggesting that LMs are proposing hypotheses without being able to actually apply the rules. Through empirical and human analyses, we further reveal several discrepancies between the inductive reasoning processes of LMs and humans, shedding light on both the potentials and limitations of using LMs in inductive reasoning tasks.",Oral 3A,https://openreview.net/pdf?id=bNt7oajl2a,https://openreview.net/forum?id=bNt7oajl2a
"['Hyosoon Jang', 'Minsu Kim', 'Sungsoo Ahn']",ICLR,Learning Energy Decompositions for Partial Inference in GFlowNets,https://iclr.cc/virtual/2024/oral/19762,2024," This paper studies generative flow networks (GFlowNets) to sample objects from the Boltzmann energy distribution via a sequence of actions. In particular, we focus on improving GFlowNet with partial inference: training flow functions with the evaluation of the intermediate states or transitions. To this end, the recently developed forward-looking GFlowNet reparameterizes the flow functions based on evaluating the energy of intermediate states. However, such an evaluation of intermediate energies may (i) be too expensive or impossible to evaluate and (ii) even provide misleading training signals under large energy fluctuations along the sequence of actions. To resolve this issue, we propose learning energy decompositions for GFlowNets (LED-GFN). Our main idea is to (i) decompose the energy of an object into learnable potential functions defined on state transitions and (ii) reparameterize the flow functions using the potential functions. In particular, to produce informative local credits, we propose to regularize the potential to change smoothly over the sequence of actions. It is also noteworthy that training GFlowNet with our learned potential can preserve the optimal policy. We empirically verify the superiority of LED-GFN in five problems including the generation of unstructured and maximum independent sets, molecular graphs, and RNA sequences.",Oral 3B,https://openreview.net/pdf?id=P15CHILQlg,https://openreview.net/forum?id=P15CHILQlg
"['Yichen Wu', 'Long-Kai Huang', 'Renzhen Wang', 'Deyu Meng', 'Ying Wei']",ICLR,Meta Continual Learning Revisited: Implicitly Enhancing Online Hessian Approximation via Variance Reduction,https://iclr.cc/virtual/2024/oral/19759,2024," Regularization-based methods have so far been among the de facto choices for continual learning. Recent theoretical studies have revealed that these methods all boil down to relying on the Hessian matrix approximation of model weights. However, these methods suffer from suboptimal trade-offs between knowledge transfer and forgetting due to fixed and unchanging Hessian estimations during training.Another seemingly parallel strand of Meta-Continual Learning (Meta-CL) algorithms enforces alignment between gradients of previous tasks and that of the current task. In this work we revisit Meta-CL and for the first time bridge it with regularization-based methods. Concretely, Meta-CL implicitly approximates Hessian in an online manner, which enjoys the benefits of timely adaptation but meantime suffers from high variance induced by random memory buffer sampling. We are thus highly motivated to combine the best of both worlds, through the proposal of Variance Reduced Meta-CL (VR-MCL) to achieve both timely and accurate Hessian approximation.Through comprehensive experiments across three datasets and various settings, we consistently observe that VR-MCL outperforms other SOTA methods, which further validates the effectiveness of VR-MCL.",Oral 3C,https://openreview.net/pdf?id=TpD2aG1h0D,https://openreview.net/forum?id=TpD2aG1h0D
"['Kim-Celine Kahl', 'Carsten Lüth', 'Maximilian Zenk', 'Klaus Maier-Hein', 'Paul F. Jaeger']",ICLR,ValUES: A Framework for Systematic Validation of Uncertainty Estimation in Semantic Segmentation,https://iclr.cc/virtual/2024/oral/19714,2024," Uncertainty estimation is an essential and heavily-studied component for the reliable application of semantic segmentation methods. While various studies exist claiming methodological advances on the one hand, and successful application on the other hand, the field is currently hampered by a gap between theory and practice leaving fundamental questions unanswered: Can data-related and model-related uncertainty really be separated in practice? Which components of an uncertainty method are essential for real-world performance? Which uncertainty method works well for which application? In this work, we link this research gap to a lack of systematic and comprehensive evaluation of uncertainty methods. Specifically, we identify three key pitfalls in current literature and present an evaluation framework that bridges the research gap by providing 1) a controlled environment for studying data ambiguities as well as distribution shifts, 2) systematic ablations of relevant method components, and 3) test-beds for the five predominant uncertainty applications: OoD-detection, active learning, failure detection, calibration, and ambiguity modeling. Empirical results on simulated as well as real-world data demonstrate how the proposed framework is able to answer the predominant questions in the field revealing for instance that 1) separation of uncertainty types works on simulated data but does not necessarily translate to real-world data, 2) aggregation of scores is a crucial but currently neglected component of uncertainty methods, 3) While ensembles are performing most robustly across the different downstream tasks and settings, test-time augmentation often constitutes a light-weight alternative. Code is at: https://github.com/IML-DKFZ/values",Oral 3D,https://openreview.net/pdf?id=yV6fD7LYkF,https://openreview.net/forum?id=yV6fD7LYkF
"['Ricky T. Q. Chen', 'Yaron Lipman']",ICLR,Flow Matching on General Geometries,https://iclr.cc/virtual/2024/oral/19740,2024," We propose Riemannian Flow Matching (RFM), a simple yet powerful framework for training continuous normalizing flows on manifolds. Existing methods for generative modeling on manifolds either require expensive simulation, are inherently unable to scale to high dimensions, or use approximations for limiting quantities that result in biased training objectives. Riemannian Flow Matching bypasses these limitations and offers several advantages over previous approaches: it is simulation-free on simple geometries, does not require divergence computation, and computes its target vector field in closed-form. The key ingredient behind RFM is the construction of a relatively simple premetric for defining target vector fields, which encompasses the existing Euclidean case. To extend to general geometries, we rely on the use of spectral decompositions to efficiently compute premetrics on the fly. Our method achieves state-of-the-art performance on real-world non-Euclidean datasets, and we demonstrate tractable training on general geometries, including triangular meshes with highly non-trivial curvature and boundaries.",Oral 4B,https://openreview.net/pdf?id=g7ohDlTITL,https://openreview.net/forum?id=g7ohDlTITL
"['Seohong Park', 'Oleh Rybkin', 'Sergey Levine']",ICLR,METRA: Scalable Unsupervised RL with Metric-Aware Abstraction,https://iclr.cc/virtual/2024/oral/19745,2024," Unsupervised pre-training strategies have proven to be highly effective in natural language processing and computer vision. Likewise, unsupervised reinforcement learning (RL) holds the promise of discovering a variety of potentially useful behaviors that can accelerate the learning of a wide array of downstream tasks. Previous unsupervised RL approaches have mainly focused on pure exploration and mutual information skill learning. However, despite the previous attempts, making unsupervised RL truly scalable still remains a major open challenge: pure exploration approaches might struggle in complex environments with large state spaces, where covering every possible transition is infeasible, and mutual information skill learning approaches might completely fail to explore the environment due to the lack of incentives. To make unsupervised RL scalable to complex, high-dimensional environments, we propose a novel unsupervised RL objective, which we call Metric-Aware Abstraction (METRA). Our main idea is, instead of directly covering the entire state space, to only cover a compact latent space $\mathcal{Z}$ that is metrically connected to the state space $\mathcal{S}$ by temporal distances. By learning to move in every direction in the latent space, METRA obtains a tractable set of diverse behaviors that approximately cover the state space, being scalable to high-dimensional environments. Through our experiments in five locomotion and manipulation environments, we demonstrate that METRA can discover a variety of useful behaviors even in complex, pixel-based environments, being the first unsupervised RL method that discovers diverse locomotion behaviors in pixel-based Quadruped and Humanoid. Our code and videos are available at https://seohong.me/projects/metra/",Oral 4C,https://openreview.net/pdf?id=c5pwL0Soay,https://openreview.net/forum?id=c5pwL0Soay
"['Gabriel Cardoso', 'Yazid Janati el idrissi', 'Sylvain Le Corff', 'Eric Moulines']",ICLR,Monte Carlo guided Denoising Diffusion models for Bayesian linear inverse problems.,https://iclr.cc/virtual/2024/oral/19729,2024," Ill-posed linear inverse problems arise frequently in various applications, from computational photography to medical imaging.A recent line of research exploits Bayesian inference with informative priors to handle the ill-posedness of such problems.Amongst such priors, score-based generative models (SGM) have recently been successfully applied to several different inverse problems.In this study, we exploit the particular structure of the prior defined by the SGM to define a sequence of intermediate linear inverse problems. As the noise level decreases, the posteriors of these inverse problems get closer to the target posterior of the original inverse problem. To sample from this sequence of posteriors, we propose the use of Sequential Monte Carlo (SMC) methods.The proposed algorithm, \algo, is shown to be theoretically grounded and we provide numerical simulations showing that it outperforms competing baselines when dealing with ill-posed inverse problems in a Bayesian setting.",Oral 4D,https://openreview.net/pdf?id=nHESwXvxWK,https://openreview.net/forum?id=nHESwXvxWK
"['Yeming Wen', 'Swarat Chaudhuri']",ICLR,Batched Low-Rank Adaptation of Foundation Models,https://iclr.cc/virtual/2024/oral/19716,2024," Low-Rank Adaptation (LoRA) has recently gained attention for fine-tuning foundation models by incorporating trainable low-rank matrices, thereby reducing the number of trainable parameters. While \lora/ offers numerous advantages, its applicability for real-time serving to a diverse and global user base is constrained by its incapability to handle multiple task-specific adapters efficiently. This imposes a performance bottleneck in scenarios requiring personalized, task-specific adaptations for each incoming request.To address this, we introduce FLoRA (Fast LoRA), a framework in which each input example in a minibatch can be associated with its unique low-rank adaptation weights, allowing for efficient batching of heterogeneous requests. We empirically demonstrate that \flora/ retains the performance merits of \lora/, showcasing competitive results on the MultiPL-E code generation benchmark spanning over 8 languages and a multilingual speech recognition task across 6 languages.",Oral 4A,https://openreview.net/pdf?id=w4abltTZ2f,https://openreview.net/forum?id=w4abltTZ2f
"['Xudong Shen', 'Chao Du', 'Tianyu Pang', 'Min Lin', 'Yongkang Wong', 'Mohan Kankanhalli']",ICLR,Finetuning Text-to-Image Diffusion Models for Fairness,https://iclr.cc/virtual/2024/oral/19734,2024," The rapid adoption of text-to-image diffusion models in society underscores an urgent need to address their biases. Without interventions, these biases could propagate a skewed worldview and restrict opportunities for minority groups. In this work, we frame fairness as a distributional alignment problem. Our solution consists of two main technical contributions: (1) a distributional alignment loss that steers specific characteristics of the generated images towards a user-defined target distribution, and (2) adjusted direct finetuning of diffusion model's sampling process (adjusted DFT), which leverages an adjusted gradient to directly optimize losses defined on the generated images. Empirically, our method markedly reduces gender, racial, and their intersectional biases for occupational prompts. Gender bias is significantly reduced even when finetuning just five soft tokens. Crucially, our method supports diverse perspectives of fairness beyond absolute equality, which is demonstrated by controlling age to a 75% young and 25% old distribution while simultaneously debiasing gender and race. Finally, our method is scalable: it can debias multiple concepts at once by simply including these prompts in the finetuning data. We share code and various fair diffusion model adaptors at https://sail-sg.github.io/finetune-fair-diffusion/.",Oral 5B,https://openreview.net/pdf?id=hnrB5YHoYu,https://openreview.net/forum?id=hnrB5YHoYu
"['Jisu Nam', 'Gyuseong Lee', 'Seonwoo Kim', 'Inès Hyeonsu Kim', 'Hyoungwon Cho', 'Seyeon Kim', 'Seungryong Kim']",ICLR,Diffusion Model for Dense Matching,https://iclr.cc/virtual/2024/oral/19751,2024," The objective for establishing dense correspondence between paired images con- sists of two terms: a data term and a prior term. While conventional techniques focused on defining hand-designed prior terms, which are difficult to formulate, re- cent approaches have focused on learning the data term with deep neural networks without explicitly modeling the prior, assuming that the model itself has the capacity to learn an optimal prior from a large-scale dataset. The performance improvement was obvious, however, they often fail to address inherent ambiguities of matching, such as textureless regions, repetitive patterns, large displacements, or noises. To address this, we propose DiffMatch, a novel conditional diffusion-based framework designed to explicitly model both the data and prior terms for dense matching. This is accomplished by leveraging a conditional denoising diffusion model that explic- itly takes matching cost and injects the prior within generative process. However, limited input resolution of the diffusion model is a major hindrance. We address this with a cascaded pipeline, starting with a low-resolution model, followed by a super-resolution model that successively upsamples and incorporates finer details to the matching field. Our experimental results demonstrate significant performance improvements of our method over existing approaches, and the ablation studies validate our design choices along with the effectiveness of each component. Code and pretrained weights are available at https://ku-cvlab.github.io/DiffMatch.",Oral 5A,https://openreview.net/pdf?id=Zsfiqpft6K,https://openreview.net/forum?id=Zsfiqpft6K
"['Qiuhao Zeng', 'Changjian Shui', 'Long-Kai Huang', 'Peng Liu', 'Xi Chen', 'Charles Ling', 'Boyu Wang']",ICLR,Latent Trajectory Learning for Limited Timestamps under Distribution Shift over Time,https://iclr.cc/virtual/2024/oral/19746,2024," Distribution shifts over time are common in real-world machine-learning applications. This scenario is formulated as Evolving Domain Generalization (EDG), where models aim to generalize well to unseen target domains in a time-varying system by learning and leveraging the underlying evolving pattern of the distribution shifts across domains. However, existing methods encounter challenges due to the limited number of timestamps (every domain corresponds to a timestamp) in EDG datasets, leading to difficulties in capturing evolving dynamics and risking overfitting to the sparse timestamps, which hampers their generalization and adaptability to new tasks. To address this limitation, we propose a novel approach SDE-EDG that collects the Infinitely Fined-Grid Evolving Trajectory (IFGET) of the data distribution with continuous-interpolated samples to bridge temporal gaps (intervals between two successive timestamps). Furthermore, by leveraging the inherent capacity of Stochastic Differential Equations (SDEs) to capture continuous trajectories, we propose their use to align SDE-modeled trajectories with IFGET across domains, thus enabling the capture of evolving distribution trends. We evaluate our approach on several benchmark datasets and demonstrate that it can achieve superior performance compared to existing state-of-the-art methods.",Oral 5C,https://openreview.net/pdf?id=bTMMNT7IdW,https://openreview.net/forum?id=bTMMNT7IdW
"['Shashank Venkataramanan', 'Mamshad Nayeem Rizve', 'Joao Carreira', 'Yuki Asano', 'Yannis Avrithis']",ICLR,Is ImageNet worth 1 video? Learning strong image encoders from 1 long unlabelled video,https://iclr.cc/virtual/2024/oral/19752,2024," Self-supervised learning has unlocked the potential of scaling up pretraining to billions of images, since annotation is unnecessary. But are we making the best use of data? How more economical can we be? In this work, we attempt to answer this question by making two contributions. First, we investigate first-person videos and introduce a Walking Tours'' dataset. These videos are high-resolution, hours-long, captured in a single uninterrupted take, depicting a large number of objects and actions with natural scene transitions. They are unlabeled and uncurated, thus realistic for self-supervision and comparable with human learning. Second, we introduce a novel self-supervised image pretraining method tailored for learning from continuous videos. Existing methods typically adapt image-based pretraining approaches to incorporate more frames. Instead, we advocate a tracking to learn to recognize'' approach. Our method called DoRA, leads to attention maps that D isc O ver and t RA ck objects over time in an end-to-end manner, using transformer cross-attention. We derive multiple views from the tracks and use them in a classical self-supervised distillation loss. Using our novel approach, a single Walking Tours video remarkably becomes a strong competitor to ImageNet for several image and video downstream tasks.",Oral 5D,https://openreview.net/pdf?id=Yen1lGns2o,https://openreview.net/forum?id=Yen1lGns2o
"['Kensen Shi', 'Joey Hong', 'Yinlin Deng', 'Pengcheng Yin', 'Manzil Zaheer', 'Charles Sutton']",ICLR,ExeDec: Execution Decomposition for Compositional Generalization in Neural Program Synthesis,https://iclr.cc/virtual/2024/oral/19726,2024," When writing programs, people have the ability to tackle a new complex task by decomposing it into smaller and more familiar subtasks. While it is difficult to measure whether neural program synthesis methods have similar capabilities, we can measure whether they compositionally generalize, that is, whether a model that has been trained on the simpler subtasks is subsequently able to solve more complex tasks. In this paper, we characterize several different forms of compositional generalization that are desirable in program synthesis, forming a meta-benchmark which we use to create generalization tasks for two popular datasets, RobustFill and DeepCoder. We then propose ExeDec, a novel decomposition-based synthesis strategy that predicts execution subgoals to solve problems step-by-step informed by program execution at each step. When used with Transformer models trained from scratch, ExeDec has better synthesis performance and greatly improved compositional generalization ability compared to baselines. Finally, we use our benchmarks to demonstrate that LLMs struggle to compositionally generalize when asked to do programming-by-example in a few-shot setting, but an ExeDec-style prompting approach can improve the generalization ability and overall performance.",Oral 6A,https://openreview.net/pdf?id=oTRwljRgiv,https://openreview.net/forum?id=oTRwljRgiv
"['Germain Kolossov', 'Andrea Montanari', 'Pulkit Tandon']",ICLR,Towards a statistical theory of data selection under weak supervision,https://iclr.cc/virtual/2024/oral/19772,2024," Given a sample of size N, it is often useful to select a subsample of smaller size n < N to be used for statistical estimation or learning. Such a data selection step is useful to reduce the requirements of data labeling and the computational complexity of learning. We assume to be given N unlabeled samples $x_{i}$, and to be given access to a 'surrogate model' that can predict labels $y_i$ better than random guessing. Our goal is to select a subset of the samples, to be denoted by {$x_{i}$}$_{i\in G}$, of size $|G|=n < N$. We then acquire labels for this set and we use them to train a model via regularized empirical risk minimization. By using a mixture of numerical experiments on real and synthetic data, and mathematical derivations under low- and high- dimensional asymptotics, we show that: (i) Data selection can be very effective, in particular beating training on the full sample in some cases; (ii) Certain popular choices in data selection methods (e.g. unbiased reweighted subsampling, or influence function-based subsampling) can be substantially suboptimal.",Oral 6C,https://openreview.net/pdf?id=HhfcNgQn6p,https://openreview.net/forum?id=HhfcNgQn6p
"['Yonatan Oren', 'Nicole Meister', 'Niladri Chatterji', 'Faisal Ladhak', 'Tatsunori Hashimoto']",ICLR,Proving Test Set Contamination in Black-Box Language Models,https://iclr.cc/virtual/2024/oral/19769,2024," Large language models are trained on vast amounts of internet data, prompting concerns that they have memorized public benchmarks. Detecting this type of contamination is challenging because the pretraining data used by proprietary models are often not publicly accessible.We propose a procedure for detecting test set contamination of language models with exact false positive guarantees and without access to pretraining data or model weights. Our approach leverages the fact that when there is no data contamination, all orderings of an exchangeable benchmark should be equally likely. In contrast, the tendency for language models to memorize example order means that a contaminated language model will find certain canonical orderings to be much more likely than others. Our test flags potential contamination whenever the likelihood of a canonically ordered benchmark dataset is significantly higher than the likelihood after shuffling the examples.We demonstrate that our procedure is sensitive enough to reliably detect contamination in challenging situations, including models as small as 1.4 billion parameters, on small test sets only 1000 examples, and datasets that appear only a few times in the pretraining corpus. Finally, we evaluate LLaMA-2 to apply our test in a realistic setting and find our results to be consistent with existing contamination evaluations.",Oral 6B,https://openreview.net/pdf?id=KS8mIvetg2,https://openreview.net/forum?id=KS8mIvetg2
"['Zengwei Yao', 'Liyong Guo', 'Xiaoyu Yang', 'Wei Kang', 'Fangjun Kuang', 'Yifan Yang', 'Zengrui Jin', 'Long Lin', 'Daniel Povey']",ICLR,Zipformer: A faster and better encoder for automatic speech recognition,https://iclr.cc/virtual/2024/oral/19784,2024," The Conformer has become the most popular encoder model for automatic speech recognition (ASR).  It adds convolution modules to a transformer to learn both local and global dependencies. In this work we describe a faster, more memory-efficient, and better-performing transformer, called Zipformer.  Modeling changes include: 1) a U-Net-like encoder structure where middle stacks operate at lower frame rates; 2) reorganized block structure with more modules, within which we re-use attention weights for efficiency; 3) a modified form of LayerNorm called BiasNorm allows us to retain some length information; 4)  new activation functions SwooshR and SwooshL work better than Swish.  We also propose a new optimizer, called ScaledAdam, which scales the update by each tensor's current scale to keep the relative change about the same, and also explictly learns the parameter scale. It achieves faster converge and better performance than Adam. Extensive experiments on LibriSpeech, Aishell-1, and WenetSpeech datasets demonstrate the effectiveness of our proposed Zipformer over other state-of-the-art ASR models. Our code is publicly available at https://github.com/k2-fsa/icefall.",Oral 6D,https://openreview.net/pdf?id=9WD9KwssyT,https://openreview.net/forum?id=9WD9KwssyT
"['Jen-tse Huang', 'Wenxuan Wang', 'Eric John Li', 'Man Ho LAM', 'Shujie Ren', 'Youliang Yuan', 'Wenxiang Jiao', 'Zhaopeng Tu', 'Michael Lyu']",ICLR,On the Humanity of Conversational AI: Evaluating the Psychological Portrayal of LLMs,https://iclr.cc/virtual/2024/oral/19775,2024," Large Language Models (LLMs) have recently showcased their remarkable capacities, not only in natural language processing tasks but also across diverse domains such as clinical medicine, legal consultation, and education. LLMs become more than mere applications, evolving into assistants capable of addressing diverse user requests. This narrows the distinction between human beings and artificial intelligence agents, raising intriguing questions regarding the potential manifestation of personalities, temperaments, and emotions within LLMs. In this paper, we propose a framework, PsychoBench, for evaluating diverse psychological aspects of LLMs. Comprising thirteen scales commonly used in clinical psychology, PsychoBench further classifies these scales into four distinct categories: personality traits, interpersonal relationships, motivational tests, and emotional abilities. Our study examines five popular models, namely text-davinci-003, ChatGPT, GPT-4, LLaMA-2-7b, and LLaMA-2-13b. Additionally, we employ a jailbreak approach to bypass the safety alignment protocols and test the intrinsic natures of LLMs. We have made PsychoBench openly accessible via https://github.com/CUHK-ARISE/PsychoBench.",Oral 7D,https://openreview.net/pdf?id=H3UayAQWoE,https://openreview.net/forum?id=H3UayAQWoE
"['Sergei Solonets', 'Daniil Sinitsyn', 'Lukas Von Stumberg', 'Nikita Araslanov', 'Daniel Cremers']",ICLR,An Analytical Solution to Gauss-Newton Loss for Direct Image Alignment,https://iclr.cc/virtual/2024/oral/19730,2024," Direct image alignment is a widely used technique for relative 6DoF pose estimation between two images, but its accuracy strongly depends on pose initialization.Therefore, recent end-to-end frameworks increase the convergence basin of the learned feature descriptors with special training objectives, such as the Gauss-Newton loss.However, the training data may exhibit bias toward a specific type of motion and pose initialization,thus limiting the generalization of these methods.In this work, we derive a closed-form solution to the expected optimum of the Gauss-Newton loss. The solution is agnostic to the underlying feature representation and allows us to dynamically adjust the basin of convergence according to our assumptions about the uncertainty in the current estimates. These properties allow for effective control over the convergence in the alignment process.Despite using self-supervised feature embeddings, our solution achieves compelling accuracy w.r.t. the state-of-the-art direct image alignment methods trained end-to-end with pose supervision, and demonstrates improved robustness to pose initialization.Our analytical solution exposes some inherent limitations of end-to-end learning with the Gauss-Newton loss, and establishes an intriguing connection between direct image alignment and feature-matching approaches.",Oral 7A,https://openreview.net/pdf?id=mE52zURNGc,https://openreview.net/forum?id=mE52zURNGc
"['Yiding Jiang', 'Christina Baek', 'J Kolter']",ICLR,"On the Joint Interaction of Models, Data, and Features",https://iclr.cc/virtual/2024/oral/19712,2024," Learning features from data is one of the defining characteristics of deep learning,but the theoretical understanding of the role features play in deep learning is still inearly development. To address this gap, we introduce a new tool, the interactiontensor, for empirically analyzing the interaction between data and model throughfeatures. With the interaction tensor, we make several key observations abouthow features are distributed in data and how models with different random seedslearn different features. Based on these observations, we propose a conceptualframework for feature learning. Under this framework, the expected accuracy for asingle hypothesis and agreement for a pair of hypotheses can both be derived inclosed form. We demonstrate that the proposed framework can explain empiricallyobserved phenomena, including the recently discovered Generalization Disagreement Equality (GDE) that allows for estimating the generalization error with onlyunlabeled data. Further, our theory also provides explicit construction of naturaldata distributions that break the GDE. Thus, we believe this work provides valuablenew insight into our understanding of feature learning.",Oral 7C,https://openreview.net/pdf?id=ze7DOLi394,https://openreview.net/forum?id=ze7DOLi394
"['Anshuman Chhabra', 'Peizhao Li', 'Prasant Mohapatra', 'Hongfu Liu']",ICLR,"""What Data Benefits My Classifier?"" Enhancing Model Performance and Interpretability through Influence-Based Data Selection",https://iclr.cc/virtual/2024/oral/19774,2024," Classification models are ubiquitously deployed in society and necessitate high utility, fairness, and robustness performance. Current research efforts mainly focus on improving model architectures and learning algorithms on fixed datasets to achieve this goal. In contrast, in this paper, we address an orthogonal yet crucial problem: given a fixed convex learning model (or a convex surrogate for a non-convex model) and a function of interest, we assess what data benefits the model by interpreting the feature space, and then aim to improve performance as measured by this function. To this end, we propose the use of influence estimation models for interpreting the classifier's performance from the perspective of the data feature space. Additionally, we propose data selection approaches based on influence that enhance model utility, fairness, and robustness. Through extensive experiments on synthetic and real-world datasets, we validate and demonstrate the effectiveness of our approaches not only for conventional classification scenarios, but also under more challenging scenarios such as distribution shifts, fairness poisoning attacks, utility evasion attacks, online learning, and active learning.",Oral 7B,https://openreview.net/pdf?id=HE9eUQlAvo,https://openreview.net/forum?id=HE9eUQlAvo
"['Hengrui Zhang', 'Jiani Zhang', 'Zhengyuan Shen', 'Balasubramaniam Srinivasan', 'Xiao Qin', 'Christos Faloutsos', 'Huzefa Rangwala', 'George Karypis']",ICLR,Mixed-Type Tabular Data Synthesis with Score-based Diffusion in Latent Space,https://iclr.cc/virtual/2024/oral/19792,2024," Recent advances in tabular data generation have greatly enhanced synthetic data quality. However, extending diffusion models to tabular data is challenging due to the intricately varied distributions and a blend of data types of tabular data. This paper introduces TabSyn, a methodology that synthesizes tabular data by leveraging a diffusion model within a variational autoencoder (VAE) crafted latent space. The key advantages of the proposed Tabsyn include (1) Generality: the ability to handle a broad spectrum of data types by converting them into a single unified space and explicitly capturing inter-column relations; (2) Quality: optimizing the distribution of latent embeddings to enhance the subsequent training of diffusion models, which helps generate high-quality synthetic data; (3) Speed: much fewer number of reverse steps and faster synthesis speed than existing diffusion-based methods. Extensive experiments on six datasets with five metrics demonstrate that Tabsyn outperforms existing methods. Specifically, it reduces the error rates by 86% and 67% for column-wise distribution and pair-wise column correlation estimations compared with the most competitive baselines. The code has been made available at https://github.com/amazon-science/tabsyn.",Oral 8A,https://openreview.net/pdf?id=4Ay23yeuz0,https://openreview.net/forum?id=4Ay23yeuz0
"['Ziheng Qin', 'Kai Wang', 'Zangwei Zheng', 'Jianyang Gu', 'Xiangyu Peng', 'Zhaopan Xu', 'Zhou Daquan', 'Lei Shang', 'Baigui Sun', 'Xuansong Xie', 'Yang You']",ICLR,InfoBatch: Lossless Training Speed Up by Unbiased Dynamic Data Pruning,https://iclr.cc/virtual/2024/oral/19779,2024," Data pruning aims to obtain lossless performances with less overall cost. A common approach is to filter out samples that make less contribution to the training. This could lead to gradient expectation bias compared to the original data. To solve this problem, we propose InfoBatch, a novel framework aiming to achieve lossless training acceleration by unbiased dynamic data pruning. Specifically, InfoBatchrandomly prunes a portion of less informative samples based on the loss distribution and rescales the gradients of the remaining samples to approximate the original gradient. As a plug-and-play and architecture-agnostic framework, InfoBatch consistently obtains lossless training results on classification, semantic segmentation, vision pertaining, and instruction fine-tuning tasks. On CIFAR10/100, ImageNet-1K, and ADE20K, InfoBatch losslessly saves 40% overall cost. For pertaining MAE and diffusion model, InfoBatch can respectively save 24.8% and 27% cost. For LLaMA instruction fine-tuning, combining InfoBatch and the recent coreset selection method (DQ) can achieve 10 times acceleration. Our results encourage more exploration on the data efficiency aspect of large model training. Code is publicly available at NUS-HPC-AI-Lab/InfoBatch.",Oral 8B,https://openreview.net/pdf?id=C61sk5LsK6,https://openreview.net/forum?id=C61sk5LsK6
"['Haiming Wang', 'Huajian Xin', 'Chuanyang Zheng', 'Zhengying Liu', 'Qingxing Cao', 'Yinya Huang', 'Jing Xiong', 'Han Shi', 'Enze Xie', 'Jian Yin', 'Zhenguo Li', 'Xiaodan Liang']",ICLR,LEGO-Prover: Neural Theorem Proving with Growing Libraries,https://iclr.cc/virtual/2024/oral/19793,2024," Despite the success of large language models (LLMs), the task of theorem proving still remains one of the hardest reasoning tasks that is far from being fully solved. Prior methods using language models have demonstrated promising results, but they still struggle to prove even middle school level theorems. One common limitation of these methods is that they assume a fixed theorem library during the whole theorem proving process. However, as we all know, creating new useful theorems or even new theories is not only helpful but crucial and necessary for advancing mathematics and proving harder and deeper results. In this work, we present LEGO-Prover, which employs a growing skill library containing verified lemmas as skills to augment the capability of LLMs used in theorem proving. By constructing the proof modularly, LEGO-Prover enables LLMs to utilize existing skills retrieved from the library and to create new skills during the proving process. These skills are further evolved (by prompting an LLM) to enrich the library on another scale. Modular and reusable skills are constantly added to the library to enable tackling increasingly intricate mathematical problems. Moreover, the learned library further bridges the gap between human proofs and formal proofs by making it easier to impute missing steps. LEGO-Prover advances the state-of-the-art pass rate on miniF2F-valid (48.0\% to 57.0\%) and miniF2F-test (45.5\% to 50.0\%). During the proving process, LEGO-Prover also generates over 20,000 skills (theorems/lemmas) and adds them to the growing library. Our ablation study indicates that these newly added skills are indeed helpful for proving theorems, resulting in a 4.9\% improvement in success rate",Oral 8C,https://openreview.net/pdf?id=3f5PALef5B,https://openreview.net/forum?id=3f5PALef5B
"['Yang He', 'Lingao Xiao', 'Joey Tianyi Zhou', 'Ivor Tsang']",ICLR,Multisize Dataset Condensation,https://iclr.cc/virtual/2024/oral/19777,2024," While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. 1) Due to the fluctuating computational resources of these devices, there's a demand for a flexible dataset size that diverges from a predefined size. 2) The limited computational power on devices often prevents additional condensation operations. These two challenges connect to the ""subset degradation problem"" in traditional dataset condensation: a subset from a larger condensed dataset is often unrepresentative compared to directly condensing the whole dataset to that smaller size. In this paper, we propose Multisize Dataset Condensation (MDC) by **compressing $N$ condensation processes into a single condensation process to obtain datasets with multiple sizes.** Specifically, we introduce an ""adaptive subset loss"" on top of the basic condensation loss to mitigate the ""subset degradation problem"". Our MDC method offers several benefits: 1) No additional condensation process is required; 2) reduced storage requirement by reusing condensed images. Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including SVHN,  CIFAR-10, CIFAR-100 and ImageNet. For example, we achieved 5.22%-6.40% average accuracy gains on condensing CIFAR-10 to ten images per class. Code is available at: [https://github.com/he-y/Multisize-Dataset-Condensation](https://github.com/he-y/Multisize-Dataset-Condensation).",Oral 8D,https://openreview.net/pdf?id=FVhmnvqnsI,https://openreview.net/forum?id=FVhmnvqnsI
"['Nathan Frey', 'Dan Berenberg', 'Karina Zadorozhny', 'Joseph Kleinhenz', 'Julien Lafrance-Vanasse', 'Isidro Hotzel', 'Yan Wu', 'Stephen Ra', 'Richard Bonneau', 'Kyunghyun Cho', 'Andreas Loukas', 'Vladimir Gligorijevic', 'Saeed Saremi']",ICLR,Protein Discovery with Discrete Walk-Jump Sampling,https://iclr.cc/virtual/2024/oral/19713,2024," We resolve difficulties in training and sampling from a discrete generative model by learning a smoothed energy function, sampling from the smoothed data manifold with Langevin Markov chain Monte Carlo (MCMC), and projecting back to the true data manifold with one-step denoising. Our $\textit{Discrete Walk-Jump Sampling}$ formalism combines the contrastive divergence training of an energy-based model and improved sample quality of a score-based model, while simplifying training and sampling by requiring only a single noise level. We evaluate the robustness of our approach on generative modeling of antibody proteins and introduce the $\textit{distributional conformity score}$ to benchmark protein generative models. By optimizing and sampling from our models for the proposed distributional conformity score, 97-100\% of generated samples are successfully expressed and purified and 70\% of functional designs show equal or improved binding affinity compared to known functional antibodies on the first attempt in a single round of laboratory experiments. We also report the first demonstration of long-run fast-mixing MCMC chains where diverse antibody protein classes are visited in a single MCMC chain.",Oral 1A,https://openreview.net/pdf?id=zMPHKOmQNb,https://openreview.net/forum?id=zMPHKOmQNb
"['Sherry Yang', 'Yilun Du', 'Seyed Ghasemipour', 'Jonathan Tompson', 'Leslie Kaelbling', 'Dale Schuurmans', 'Pieter Abbeel']",ICLR,Learning Interactive Real-World Simulators,https://iclr.cc/virtual/2024/oral/19722,2024," Generative models trained on internet data have revolutionized how text, image, and video content can be created. Perhaps the next milestone for generative models is to simulate realistic experience in response to actions taken by humans, robots, and other interactive agents. Applications of a real-world simulator range from controllable content creation in games and movies, to training embodied agents purely in simulation that can be directly deployed in the real world. We explore the possibility of learning a universal simulator (UniSim) of real-world interaction through generative modeling. We first make the important observation that natural datasets available for learning a real-world simulator are often rich along different axes (e.g., abundant objects in image data, densely sampled actions in robotics data, and diverse movements in navigation data). With careful orchestration of diverse datasets, each providing a different aspect of the overall experience, UniSim can emulate how humans and agents interact with the world by simulating the visual outcome of both high-level instructions such as “open the drawer” and low-level controls such as “move by x,y” from otherwise static scenes and objects. There are numerous use cases for such a real-world simulator. As an example, we use UniSim to train both high-level vision-language planners and low-level reinforcement learning policies, each of which exhibit zero-shot real-world transfer after training purely in a learned real-world simulator. We also show that other types of intelligence such as video captioning models can benefit from training with simulated experience in UniSim, opening up even wider applications.",Oral 1B,https://openreview.net/pdf?id=sFyTZEqmUY,https://openreview.net/forum?id=sFyTZEqmUY
"['Suyu Ge', 'Yunan Zhang', 'Liyuan Liu', 'Minjia Zhang', 'Jiawei Han', 'Jianfeng Gao']",ICLR,Model Tells You What to Discard: Adaptive KV Cache Compression for LLMs,https://iclr.cc/virtual/2024/oral/19718,2024," In this study, we introduce adaptive KV cache compression, a plug-and-play method that reduces the memory footprint of generative inference for Large Language Models (LLMs). Different from the conventional KV cache that retains key and value vectors for all context tokens, we conduct targeted profiling to discern the intrinsic structure of attention modules. Based on the recognized structure, we then construct the KV cache in an adaptive manner: evicting long-range contexts on attention heads emphasizing local contexts, discarding non-special tokens on attention heads centered on special tokens, and only employing the standard KV cache for attention heads that broadly attend to all tokens. Moreover, with the lightweight attention profiling used to guide the construction of the adaptive KV cache, FastGen can be deployed without resource-intensive fine-tuning or re-training. In our experiments across various asks, FastGen demonstrates substantial reduction on GPU memory consumption with negligible generation quality loss. We will release our code and the compatible CUDA kernel for reproducibility.",Oral 1C,https://openreview.net/pdf?id=uNrFpDPMyo,https://openreview.net/forum?id=uNrFpDPMyo
"['Yicong Hong', 'Kai Zhang', 'Jiuxiang Gu', 'Sai Bi', 'Yang Zhou', 'Difan Liu', 'Feng Liu', 'Kalyan Sunkavalli', 'Trung Bui', 'Hao Tan']",ICLR,LRM: Large Reconstruction Model for Single Image to 3D,https://iclr.cc/virtual/2024/oral/19721,2024," We propose the first Large Reconstruction Model (LRM) that predicts the 3D model of an object from a single input image within just 5 seconds. In contrast to many previous methods that are trained on small-scale datasets such as ShapeNet in a category-specific fashion, LRM adopts a highly scalable transformer-based architecture with 500 million learnable parameters to directly predict a neural radiance field (NeRF) from the input image. We train our model in an end-to-end manner on massive multi-view data containing around 1 million objects, including both synthetic renderings from Objaverse and real captures from MVImgNet. This combination of a high-capacity model and large-scale training data empowers our model to be highly generalizable and produce high-quality 3D reconstructions from various testing inputs, including real-world in-the-wild captures and images created by generative models. Video demos and interactable 3D meshes can be found on our LRM project webpage: https://yiconghong.me/LRM.",Oral 2B,https://openreview.net/pdf?id=sllU8vvsFF,https://openreview.net/forum?id=sllU8vvsFF
"['Akari Asai', 'Zeqiu Wu', 'Yizhong Wang', 'Avi Sil', 'Hannaneh Hajishirzi']",ICLR,"Self-RAG: Learning to Retrieve, Generate, and Critique through Self-Reflection",https://iclr.cc/virtual/2024/oral/19736,2024," Despite their remarkable capabilities, large language models (LLMs) often produce responses containing factual inaccuracies due to their sole reliance on the parametric knowledge they encapsulate. Retrieval-Augmented Generation (RAG), an ad hoc approach that augments LMs with retrieval of relevant knowledge, decreases such issues. However, indiscriminately retrieving and incorporating a fixed number of retrieved passages, regardless of whether retrieval is necessary, or passages are relevant, diminishes LM versatility or can lead to unhelpful response generation. We introduce a new framework called Self-Reflective Retrieval-Augmented Generation (Self-RAG) that enhances an LM's quality and factuality through retrieval and self-reflection. Our framework trains a single arbitrary LM that adaptively retrieves passages on-demand, and generates and reflects on retrieved passages and its generations using special tokens, called {\it reflection} tokens. Generating reflection tokens makes the LM controllable during the inference phase, enabling it to tailor its behavior to diverse task requirements. Experiments show that Self-RAG (7B and 13B parameters) significantly outperforms state-of-the-art LLMs and retrieval-augmented models on a diverse set of tasks. Specifically, Self-RAG outperforms ChatGPT and retrieval-augmented Llama2-chat on Open-domain QA, reasoning, and fact verification tasks, and it shows significant gains in improving factuality and citation accuracy for long-form generations relative to these models. Our code and trained models are available at https://selfrag.github.io/",Oral 2A,https://openreview.net/pdf?id=hSyW5go0v8,https://openreview.net/forum?id=hSyW5go0v8
"['Iman Mirzadeh', 'Keivan Alizadeh-Vahid', 'Sachin Mehta', 'Carlo C del Mundo', 'Oncel Tuzel', 'Golnoosh Samei', 'Mohammad Rastegari', 'Mehrdad Farajtabar']",ICLR,ReLU Strikes Back: Exploiting Activation Sparsity in Large Language Models,https://iclr.cc/virtual/2024/oral/19725,2024," Large Language Models (LLMs) with billions of parameters have drastically transformed AI applications. However, their demanding computation during inference has raised significant challenges for deployment on resource-constrained devices. Despite recent trends favoring alternative activation functions such as GELU or SiLU, known for increased computation, this study strongly advocates for reinstating ReLU activation in LLMs. We demonstrate that using the ReLU activation function has a negligible impact on convergence and performance while significantly reducing computation and weight transfer. This reduction is particularly valuable during the memory-bound inference step, where efficiency is paramount. Exploring sparsity patterns in ReLU-based LLMs, we unveil the reutilization of activated neurons for generating new tokens and leveraging these insights, we propose practical strategies to substantially reduce LLM inference computation up to three times, using ReLU activations with minimal performance trade-offs.",Oral 3A,https://openreview.net/pdf?id=osoWxY8q2E,https://openreview.net/forum?id=osoWxY8q2E
"['Pablo Pernías', 'Dominic Rampas', 'Mats L. Richter', 'Christopher Pal', 'Marc Aubreville']",ICLR,Würstchen: An Efficient Architecture for Large-Scale Text-to-Image Diffusion Models,https://iclr.cc/virtual/2024/oral/19738,2024," We introduce Würstchen, a novel architecture for text-to-image synthesis that combines competitive performance with unprecedented cost-effectiveness for large-scale text-to-image diffusion models.A key contribution of our work is to develop a latent diffusion technique in which we learn a detailed but extremely compact semantic image representation used to guide the diffusion process. This highly compressed representation of an image provides much more detailed guidance compared to latent representations of language and this significantly reduces the computational requirements to achieve state-of-the-art results. Our approach also improves the quality of text-conditioned image generation based on our user preference study.The training requirements of our approach consists of 24,602 A100-GPU hours - compared to Stable Diffusion 2.1's 200,000 GPU hours.  Our approach also requires less training data to achieve these results. Furthermore, our compact latent representations allows us to perform inference over twice as fast, slashing the usual costs and carbon footprint of a state-of-the-art (SOTA) diffusion model significantly, without compromising the end performance. In a broader comparison against SOTA models our approach is substantially more efficient and compares favourably in terms of image quality.We believe that this work motivates more emphasis on the prioritization of both performance and computational accessibility.",Oral 2C,https://openreview.net/pdf?id=gU58d5QeGv,https://openreview.net/forum?id=gU58d5QeGv
"['Jason Zhang', 'Amy Lin', 'Moneish Kumar', 'Tzu-Hsuan Yang', 'Deva Ramanan', 'Shubham Tulsiani']",ICLR,Cameras as Rays: Pose Estimation via Ray Diffusion,https://iclr.cc/virtual/2024/oral/19778,2024," Estimating camera poses is a fundamental task for 3D reconstruction and remains challenging given sparsely sampled views (<10). In contrast to existing approaches that pursue top-down prediction of global parametrizations of camera extrinsics, we propose a distributed representation of camera pose that treats a camera as a bundle of rays. This representation allows for a tight coupling with spatial image features improving pose precision. We observe that this representation is naturally suited for set-level transformers and develop a regression-based approach that maps image patches to corresponding rays. To capture the inherent uncertainties in sparse-view pose inference, we adapt this approach to learn a denoising diffusion model which allows us to sample plausible modes while improving performance. Our proposed methods, both regression- and diffusion-based, demonstrate state-of-the-art performance on camera pose estimation on CO3D while generalizing to unseen object categories and in-the-wild captures.",Oral 3B,https://openreview.net/pdf?id=EanCFCwAjM,https://openreview.net/forum?id=EanCFCwAjM
"['Miltiadis (Miltos) Kofinas', 'Boris Knyazev', 'Yan Zhang', 'Yunlu Chen', 'Gertjan J Burghouts', 'Efstratios Gavves', 'Cees G Snoek', 'David Zhang']",ICLR,Graph Neural Networks for Learning Equivariant Representations of Neural Networks,https://iclr.cc/virtual/2024/oral/19727,2024," Neural networks that process the parameters of other neural networks find applications in domains as diverse as classifying implicit neural representations, generating neural network weights, and predicting generalization errors. However, existing approaches either overlook the inherent permutation symmetry in the neural network or rely on intricate weight-sharing patterns to achieve equivariance, while ignoring the impact of the network architecture itself. In this work, we propose to represent neural networks as computational graphs of parameters, which allows us to harness powerful graph neural networks and transformers that preserve permutation symmetry. Consequently, our approach enables a single model to encode neural computational graphs with diverse architectures. We showcase the effectiveness of our method on a wide range of tasks, including classification and editing of implicit neural representations, predicting generalization performance, and learning to optimize, while consistently outperforming state-of-the-art methods. The source code is open-sourced at https://github.com/mkofinas/neural-graphs.",Oral 4B,https://openreview.net/pdf?id=oO6FsMyDBt,https://openreview.net/forum?id=oO6FsMyDBt
"['Ian Gemp', 'Luke Marris', 'Georgios Piliouras']",ICLR,Approximating Nash Equilibria in Normal-Form Games via Stochastic Optimization,https://iclr.cc/virtual/2024/oral/19744,2024," We propose the first loss function for approximate Nash equilibria of normal-form games that is amenable to unbiased Monte Carlo estimation. This construction allows us to deploy standard non-convex stochastic optimization techniques for approximating Nash equilibria, resulting in novel algorithms  with provable guarantees. We complement our theoretical analysis with experiments demonstrating that stochastic gradient descent can outperform previous state-of-the-art approaches.",Oral 3C,https://openreview.net/pdf?id=cc8h3I3V4E,https://openreview.net/forum?id=cc8h3I3V4E
"['Haoqi Yuan', 'Zhancun Mu', 'Feiyang Xie', 'Zongqing Lu']",ICLR,Pre-Training Goal-based Models for Sample-Efficient Reinforcement Learning,https://iclr.cc/virtual/2024/oral/19728,2024," Pre-training on task-agnostic large datasets is a promising approach for enhancing the sample efficiency of reinforcement learning (RL) in solving complex tasks. We present PTGM, a novel method that pre-trains goal-based models to augment RL by providing temporal abstractions and behavior regularization. PTGM involves pre-training a low-level, goal-conditioned policy and training a high-level policy to generate goals for subsequent RL tasks. To address the challenges posed by the high-dimensional goal space, while simultaneously maintaining the agent's capability to accomplish various skills, we propose clustering goals in the dataset to form a discrete high-level action space. Additionally, we introduce a pre-trained goal prior model to regularize the behavior of the high-level policy in RL, enhancing sample efficiency and learning stability. Experimental results in a robotic simulation environment and the challenging open-world environment of Minecraft demonstrate PTGM’s superiority in sample efficiency and task performance compared to baselines. Moreover, PTGM exemplifies enhanced interpretability and generalization of the acquired low-level skills.",Oral 4C,https://openreview.net/pdf?id=o2IEmeLL9r,https://openreview.net/forum?id=o2IEmeLL9r
"['André F. Cruz', 'Moritz Hardt']",ICLR,Unprocessing Seven Years of Algorithmic Fairness,https://iclr.cc/virtual/2024/oral/19731,2024," Seven years ago, researchers proposed a postprocessing method to equalize the error rates of a model across different demographic groups. The work launched hundreds of papers purporting to improve over the postprocessing baseline. We empirically evaluate these claims through thousands of model evaluations on several tabular datasets. We find that the fairness-accuracy Pareto frontier achieved by postprocessing contains all other methods we were feasibly able to evaluate. In doing so, we address two common methodological errors that have confounded previous observations. One relates to the comparison of methods with different unconstrained base models. The other concerns methods achieving different levels of constraint relaxation. At the heart of our study is a simple idea we call unprocessing that roughly corresponds to the inverse of postprocessing. Unprocessing allows for a direct comparison of methods using different underlying models and levels of relaxation.",Oral 5B,https://openreview.net/pdf?id=jr03SfWsBS,https://openreview.net/forum?id=jr03SfWsBS
"['Tianrong Chen', 'Jiatao Gu', 'Laurent Dinh', 'Evangelos Theodorou', 'Joshua Susskind', 'Shuangfei Zhai']",ICLR,Generative Modeling with Phase Stochastic Bridge,https://iclr.cc/virtual/2024/oral/19720,2024," Diffusion models (DMs) represent state-of-the-art generative models for continuous inputs. DMs work by constructing a Stochastic Differential Equation (SDE) in the input space (ie, position space), and using a neural network to reverse it. In this work, we introduce a novel generative modeling framework grounded in \textbf{phase space dynamics}, where a phase space is defined as {an augmented space encompassing both position and velocity.} Leveraging insights from Stochastic Optimal Control, we construct a path measure in the phase space that enables efficient sampling. {In contrast to DMs, our framework demonstrates the capability to generate realistic data points at an early stage of dynamics propagation.} This early prediction sets the stage for efficient data generation by leveraging additional velocity information along the trajectory. On standard image generation benchmarks, our model yields favorable performance over baselines in the regime of small Number of Function Evaluations (NFEs). Furthermore, our approach rivals the performance of diffusion models equipped with efficient sampling techniques, underscoring its potential as a new tool generative modeling.",Oral 5A,https://openreview.net/pdf?id=tUtGjQEDd4,https://openreview.net/forum?id=tUtGjQEDd4
"['Sebastian Pineda Arango', 'Fabio Ferreira', 'Arlind Kadra', 'Frank Hutter', 'Josif Grabocka']",ICLR,Quick-Tune: Quickly Learning Which Pretrained Model to Finetune and How,https://iclr.cc/virtual/2024/oral/19719,2024," With the ever-increasing number of pretrained models, machine learning practitioners are continuously faced with which pretrained model to use, and how to finetune it for a new dataset. In this paper, we propose a methodology that jointly searches for the optimal pretrained model and the hyperparameters for finetuning it. Our method transfers knowledge about the performance of many pretrained models with multiple hyperparameter configurations on a series of datasets. To this aim, we evaluated over 20k hyperparameter configurations for finetuning 24 pretrained image classification models on 87 datasets to generate a large-scale meta-dataset. We meta-learn a gray-box performance predictor on the learning curves of this meta-dataset and use it for fast hyperparameter optimization on new datasets. We empirically demonstrate that our resulting approach can quickly select an accurate pretrained model for a new dataset together with its optimal hyperparameters.",Oral 5C,https://openreview.net/pdf?id=tqh1zdXIra,https://openreview.net/forum?id=tqh1zdXIra
"['Pascal Chang', 'Jingwei Tang', 'Markus Gross', 'Vinicius Da Costa De Azevedo']",ICLR,How I Warped Your Noise: a Temporally-Correlated Noise Prior for Diffusion Models,https://iclr.cc/virtual/2024/oral/19723,2024," Video editing and generation methods often rely on pre-trained image-based diffusion models. During the diffusion process, however, the reliance on rudimentary noise sampling techniques that do not preserve correlations present in subsequent frames of a video is detrimental to the quality of the results. This either produces high-frequency flickering, or texture-sticking artifacts that are not amenable to post-processing. With this in mind, we propose a novel method for preserving temporal correlations in a sequence of noise samples. This approach is materialized by a novel noise representation, dubbed $\int$-noise (integral noise), that reinterprets individual noise samples as a continuously integrated noise field: pixel values do not represent discrete values, but are rather the integral of an underlying infinite-resolution noise over the pixel area. Additionally, we propose a carefully tailored transport method that uses $\int$-noise to accurately advect noise samples over a sequence of frames, maximizing the correlation between different frames while also preserving the noise properties. Our results demonstrate that the proposed $\int$-noise can be used for a variety of tasks, such as video restoration, surrogate rendering, and conditional video generation.",Oral 6A,https://openreview.net/pdf?id=pzElnMrgSD,https://openreview.net/forum?id=pzElnMrgSD
"['Ido Amos', 'Jonathan Berant', 'Ankit Gupta']",ICLR,Never Train from Scratch: Fair Comparison of Long-Sequence Models Requires Data-Driven Priors,https://iclr.cc/virtual/2024/oral/19761,2024," Modeling long-range dependencies across sequences is a longstanding goal in machine learning and has led to architectures, such as state space models, that dramatically outperform Transformers on long sequences. However, these impressive empirical gains have been by and large demonstrated on benchmarks (e.g. Long Range Arena), where models are randomly initialized and trained to predict a target label from an input sequence. In this work, we show that random initialization leads to gross overestimation of the differences between architectures and that pretraining with standard denoising objectives, using only the downstream task data , leads to dramatic gains across multiple architectures and to very small gaps between Transformers and state space models (SSMs). In stark contrast to prior works, we find vanilla Transformers to match the performance of S4 on Long Range Arena when properly pretrained, and we improve the best reported results of SSMs on the PathX-256 task by 20 absolute points. Subsequently, we analyze the utility of previously-proposed structured parameterizations for SSMs and show they become mostly redundant in the presence of data-driven initialization obtained through pretraining. Our work shows that, when evaluating different architectures on supervised tasks, incorporation of data-driven priors via pretraining is essential for reliable performance estimation, and can be done efficiently.",Oral 6C,https://openreview.net/pdf?id=PdaPky8MUn,https://openreview.net/forum?id=PdaPky8MUn
"['Ahmad Faiz', 'Sotaro Kaneda', 'Ruhan Wang', 'Rita Osi', 'Prateek Sharma', 'Fan Chen', 'Lei Jiang']",ICLR,LLMCarbon: Modeling the End-to-End Carbon Footprint of Large Language Models,https://iclr.cc/virtual/2024/oral/19750,2024," The carbon footprint associated with large language models (LLMs) is a significant concern, encompassing emissions from their training, inference, experimentation, and storage processes, including operational and embodied carbon emissions. An essential aspect is accurately estimating the carbon impact of emerging LLMs even before their training, which heavily relies on GPU usage. Existing studies have reported the carbon footprint of LLM training, but only one tool, mlco2, can predict the carbon footprint of new neural networks prior to physical training. However, mlco2 has several serious limitations. It cannot extend its estimation to dense or mixture-of-experts (MoE) LLMs, disregards critical architectural parameters, focuses solely on GPUs, and cannot model embodied carbon footprints. Addressing these gaps, we introduce \textit{\carb}, an end-to-end carbon footprint projection model designed for both dense and MoE LLMs. Compared to mlco2, \carb~significantly enhances the accuracy of carbon footprint estimations for various LLMs. The source code is released at \url{https://github.com/SotaroKaneda/MLCarbon}.",Oral 6B,https://openreview.net/pdf?id=aIok3ZD9to,https://openreview.net/forum?id=aIok3ZD9to
"['Shangbin Feng', 'Weijia Shi', 'Yuyang Bai', 'Vidhisha Balachandran', 'Tianxing He', 'Yulia Tsvetkov']",ICLR,Knowledge Card: Filling LLMs' Knowledge Gaps with Plug-in Specialized Language Models,https://iclr.cc/virtual/2024/oral/19753,2024," By design, large language models (LLMs) are static general-purpose models, expensive to retrain or update frequently. As they are increasingly adopted for knowledge-intensive tasks, it becomes evident that these design choices lead to failures to generate factual, relevant, and up-to-date knowledge. To this end, we propose Knowledge Card, a modular framework to plug in new factual and relevant knowledge into general-purpose LLMs. We first introduce knowledge cards---specialized language models trained on corpora from specific domains and sources. Knowledge cards serve as parametric repositories that are selected at inference time to generate background knowledge for the base LLM. We then propose three content selectors to dynamically select and retain information in documents generated by knowledge cards, specifically controlling for relevance, brevity, and factuality of outputs. Finally, we propose two complementary integration approaches to augment the base LLM with the (relevant, factual) knowledge curated from the specialized LMs. Through extensive experiments, we demonstrate that Knowledge Card achieves state-of-the-art performance on six benchmark datasets. Ultimately, Knowledge Card framework enables dynamic synthesis and updates of knowledge from diverse domains. Its modularity will ensure that relevant knowledge can be continuously updated through the collective efforts of the research community.",Oral 7B,https://openreview.net/pdf?id=WbWtOYIzIK,https://openreview.net/forum?id=WbWtOYIzIK
"['Yubo Zhuang', 'Xiaohui Chen', 'Yun Yang', 'Richard Zhang']",ICLR,Statistically Optimal $K$-means Clustering via Nonnegative Low-rank Semidefinite Programming,https://iclr.cc/virtual/2024/oral/19717,2024," $K$-means clustering is a widely used machine learning method for identifying patterns in large datasets. Recently, semidefinite programming (SDP) relaxations have been proposed for solving the $K$-means optimization problem, which enjoy strong statistical optimality guarantees. However, the prohibitive cost of implementing an SDP solver renders these guarantees inaccessible to practical datasets. In contrast, nonnegative matrix factorization (NMF) is a simple clustering algorithm widely used by machine learning practitioners, but it lacks a solid statistical underpinning and theoretical guarantees. In this paper, we consider an NMF-like algorithm that solves a nonnegative low-rank restriction of the SDP-relaxed $K$-means formulation using a nonconvex Burer--Monteiro factorization approach. The resulting algorithm is as simple and scalable as state-of-the-art NMF algorithms while also enjoying the same strong statistical optimality guarantees as the SDP. In our experiments, we observe that our algorithm achieves significantly smaller mis-clustering errors compared to the existing state-of-the-art while maintaining scalability.",Oral 7A,https://openreview.net/pdf?id=v7ZPwoHU1j,https://openreview.net/forum?id=v7ZPwoHU1j
"['Yapei Chang', 'Kyle Lo', 'Tanya Goyal', 'Mohit Iyyer']",ICLR,BooookScore: A systematic exploration of book-length summarization in the era of LLMs,https://iclr.cc/virtual/2024/oral/19789,2024," Summarizing book-length documents ($>$100K tokens)  that exceed the context window size of large language models (LLMs) requires first breaking the input document into smaller chunks and then prompting an LLM to merge, update, and compress chunk-level summaries. Despite the complexity and importance of this task, it has yet to be meaningfully studied due to the challenges of evaluation: existing book-length summarization datasets (e.g., BookSum) are in the pretraining data of most public LLMs, and existing evaluation methods struggle to capture errors made by modern LLM summarizers. In this paper, we present the first study of the coherence of LLM-based book-length summarizers implemented via two prompting workflows: (1) hierarchically merging chunk-level summaries, and (2) incrementally updating a running summary. We obtain 1193 fine-grained human annotations on GPT-4 generated summaries of 100 recently-published books and identify eight common types of coherence errors made by LLMs. Because human evaluation is expensive and time-consuming, we develop an automatic metric, BooookScore, that measures the proportion of sentences in a summary that do not contain any of the identified error types. BooookScore has high agreement with human annotations and allows us to systematically evaluate the impact of many other critical parameters (e.g., chunk size, base LLM) while saving \$15K USD and 500 hours in human evaluation costs. We find that closed-source LLMs such as GPT-4 and Claude 2 produce summaries with higher BooookScore than those generated by open-source models. While LLaMA 2 falls behind other models, Mixtral achieves performance on par with GPT-3.5-Turbo. Incremental updating yields lower BooookScore but higher level of detail than hierarchical merging, a trade-off sometimes preferred by annotators. We release code and annotations to spur more principled research on book-length summarization.",Oral 8D,https://openreview.net/pdf?id=7Ttk3RzDeu,https://openreview.net/forum?id=7Ttk3RzDeu
"['Yuxin Wen', 'Yuchen Liu', 'Chen Chen', 'Lingjuan Lyu']",ICLR,"Detecting, Explaining, and Mitigating Memorization in Diffusion Models",https://iclr.cc/virtual/2024/oral/19787,2024," Recent breakthroughs in diffusion models have exhibited exceptional image-generation capabilities. However, studies show that some outputs are merely replications of training data. Such replications present potential legal challenges for model owners, especially when the generated content contains proprietary information. In this work, we introduce a straightforward yet effective method for detecting memorized prompts by inspecting the magnitude of text-conditional predictions. Our proposed method seamlessly integrates without disrupting sampling algorithms, and delivers high accuracy even at the first generation step, with a single generation per prompt. Building on our detection strategy, we unveil an explainable approach that shows the contribution of individual words or tokens to memorization. This offers an interactive medium for users to adjust their prompts. Moreover, we propose two strategies i.e., to mitigate memorization by leveraging the magnitude of text-conditional predictions, either through minimization during inference or filtering during training. These proposed strategies effectively counteract memorization while maintaining high-generation quality. Code is available at https://github.com/YuxinWenRick/diffusion_memorization.",Oral 8A,https://openreview.net/pdf?id=84n3UwkH7b,https://openreview.net/forum?id=84n3UwkH7b
"['Atsushi Shimizu', 'Xiaoou Cheng', 'Christopher Musco', 'Jonathan Weare']",ICLR,Improved Active Learning via Dependent Leverage Score Sampling,https://iclr.cc/virtual/2024/oral/19770,2024," We show how to obtain improved active learning methods in the agnostic (adversarial noise) setting by combining marginal leverage score sampling with non-independent sampling strategies that promote spatial coverage. In particular, we propose an easily implemented method based on the \emph{pivotal sampling algorithm}, which we test on problems motivated by learning-based methods for parametric PDEs and uncertainty quantification. In comparison to independent sampling, our method reduces the number of samples needed to reach a given target accuracy by up to $50\%$.We support our findings with two theoretical results. First, we show that any non-independent leverage score sampling method that obeys a weak \emph{one-sided $\ell_{\infty}$ independence condition} (which includes pivotal sampling) can actively learn $d$ dimensional linear functions with $O(d\log d)$ samples, matching independent sampling. This result extends recent work on matrix Chernoff bounds under $\ell_{\infty}$ independence, and may be of interest for analyzing other sampling strategies beyond pivotal sampling. Second, we show that, for the important case of polynomial regression, our pivotal method obtains an improved bound of $O(d)$ samples.",Oral 8B,https://openreview.net/pdf?id=IYxDy2jDFL,https://openreview.net/forum?id=IYxDy2jDFL
